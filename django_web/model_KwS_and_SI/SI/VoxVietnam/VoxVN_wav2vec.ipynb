{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:28:39.839937Z",
     "iopub.status.busy": "2025-10-15T07:28:39.839323Z",
     "iopub.status.idle": "2025-10-15T07:28:55.872709Z",
     "shell.execute_reply": "2025-10-15T07:28:55.871729Z",
     "shell.execute_reply.started": "2025-10-15T07:28:39.839905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers huggingface_hub httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-15T07:28:55.874660Z",
     "iopub.status.busy": "2025-10-15T07:28:55.874405Z",
     "iopub.status.idle": "2025-10-15T07:29:20.283389Z",
     "shell.execute_reply": "2025-10-15T07:29:20.282609Z",
     "shell.execute_reply.started": "2025-10-15T07:28:55.874640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import Wav2Vec2Model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "root = \"/kaggle/input/voxvn-api491/train_small_wav\"\n",
    "root_test = \"/kaggle/input/voxvietnam/wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:29:20.284765Z",
     "iopub.status.busy": "2025-10-15T07:29:20.284162Z",
     "iopub.status.idle": "2025-10-15T07:29:32.039307Z",
     "shell.execute_reply": "2025-10-15T07:29:32.038512Z",
     "shell.execute_reply.started": "2025-10-15T07:29:20.284738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_all_wavs(root):\n",
    "    all_wavs = []\n",
    "    spk2label = {}\n",
    "\n",
    "    spk_folders = sorted([\n",
    "        d for d in os.listdir(root)\n",
    "        if os.path.isdir(os.path.join(root, d))\n",
    "    ])\n",
    "\n",
    "    for label, spk_folder in enumerate(spk_folders):\n",
    "        spk2label[spk_folder] = label\n",
    "        fpath = os.path.join(root, spk_folder)\n",
    "\n",
    "        for w in sorted(os.listdir(fpath)):\n",
    "            if w.endswith(\".wav\"):\n",
    "                rel_path = f\"{spk_folder}/{w}\"\n",
    "                all_wavs.append((rel_path, label))\n",
    "\n",
    "    return all_wavs, spk2label\n",
    "\n",
    "import csv\n",
    "\n",
    "def read_csv_paths(csv_file):\n",
    "    data = []\n",
    "    label_counts = {}\n",
    "\n",
    "    with open(csv_file, \"r\", newline='', encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader, start=1):\n",
    "            line = row[0]\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) != 3:\n",
    "                raise ValueError(f\"{line}\")\n",
    "            label, path1, path2 = parts\n",
    "\n",
    "            try:\n",
    "                label = int(label)\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"Label not interger {i}: {label}\")\n",
    "\n",
    "            data.append((label, path1, path2))\n",
    "\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += 1\n",
    "            else:\n",
    "                label_counts[label] = 1\n",
    "\n",
    "    return data, label_counts\n",
    "\n",
    "\n",
    "\n",
    "all_files, tra_spk = list_all_wavs(root)\n",
    "print(len(all_files), len(tra_spk))\n",
    "print(list(all_files)[:10])\n",
    "\n",
    "o_files, o_c = read_csv_paths(\"/kaggle/input/voxvietnam/test_list_gt.csv\")\n",
    "print(len(o_files), o_c)\n",
    "\n",
    "print(o_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:29:32.041202Z",
     "iopub.status.busy": "2025-10-15T07:29:32.040980Z",
     "iopub.status.idle": "2025-10-15T07:29:32.055606Z",
     "shell.execute_reply": "2025-10-15T07:29:32.054862Z",
     "shell.execute_reply.started": "2025-10-15T07:29:32.041184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SpeakerClassificationFeatureDataset(Dataset):\n",
    "    def __init__(self, root, file_list, fixed_len=64000, one_hot=True,\n",
    "                 augment=False, sample_rate=16000, si=16):\n",
    "        self.root = root\n",
    "        self.fixed_len = fixed_len\n",
    "        self.one_hot = one_hot\n",
    "        self.augment = augment\n",
    "        self.sample_rate = sample_rate\n",
    "        self.si = si\n",
    "\n",
    "        self.data = [(label, os.path.join(root, path)) for label, path in file_list]\n",
    "        self.num_speakers = max(label for label, _ in file_list) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, path = self.data[idx]\n",
    "\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        wav = wav.mean(dim=0)\n",
    "\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            wav = resampler(wav.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                wav = wav * random.uniform(0.9, 1.1)\n",
    "            if random.random() < 0.5:\n",
    "                wav = wav + torch.randn_like(wav) * 0.005\n",
    "\n",
    "        wav = self._fix_length(wav).unsqueeze(0)\n",
    "\n",
    "        if self.one_hot:\n",
    "            lbl = torch.zeros(self.num_speakers)\n",
    "            lbl[label] = 1.0\n",
    "        else:\n",
    "            lbl = label\n",
    "\n",
    "        return wav, lbl\n",
    "\n",
    "    def _fix_length(self, wav):\n",
    "        L = wav.size(0)\n",
    "        if L > self.fixed_len:\n",
    "            start = (L - self.fixed_len) // 2\n",
    "            return wav[start:start + self.fixed_len]\n",
    "        elif L < self.fixed_len:\n",
    "            return F.pad(wav, (0, self.fixed_len - L))\n",
    "        return wav\n",
    "\n",
    "\n",
    "class SiameseSpeakerFeatureDataset(Dataset):\n",
    "    def __init__(self, root, file_list, fixed_len=64000, sample_rate=16000, si=16):\n",
    "        self.root = root\n",
    "        self.file_list = file_list\n",
    "        self.fixed_len = fixed_len\n",
    "        self.sample_rate = sample_rate\n",
    "        self.si = si\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, rel1, rel2 = self.file_list[idx]\n",
    "        p1 = os.path.join(self.root, rel1)\n",
    "        p2 = os.path.join(self.root, rel2)\n",
    "\n",
    "        wav1, sr1 = torchaudio.load(p1)\n",
    "        wav2, sr2 = torchaudio.load(p2)\n",
    "        wav1 = wav1.mean(dim=0)\n",
    "        wav2 = wav2.mean(dim=0)\n",
    "\n",
    "        if sr1 != self.sample_rate:\n",
    "            wav1 = torchaudio.transforms.Resample(sr1, self.sample_rate)(wav1.unsqueeze(0)).squeeze(0)\n",
    "        if sr2 != self.sample_rate:\n",
    "            wav2 = torchaudio.transforms.Resample(sr2, self.sample_rate)(wav2.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        wav1 = self._fix_length(wav1).unsqueeze(0)\n",
    "        wav2 = self._fix_length(wav2).unsqueeze(0)\n",
    "\n",
    "        return wav1, wav2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def _fix_length(self, wav):\n",
    "        L = wav.size(0)\n",
    "        if L > self.fixed_len:\n",
    "            start = (L - self.fixed_len) // 2\n",
    "            return wav[start:start + self.fixed_len]\n",
    "        elif L < self.fixed_len:\n",
    "            return F.pad(wav, (0, self.fixed_len - L))\n",
    "        return wav\n",
    "\n",
    "def siamese_collate_fn(batch):\n",
    "    anchors, pairs, labels = zip(*batch)\n",
    "    anchors = torch.stack(anchors)   # [B, T]\n",
    "    pairs = torch.stack(pairs)       # [B, T]\n",
    "    labels = torch.tensor(labels).float()  # [B]\n",
    "    return anchors, pairs, labels\n",
    "\n",
    "def collate_fn_classification(batch):\n",
    "    wavs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    labels = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    return wavs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:29:32.056775Z",
     "iopub.status.busy": "2025-10-15T07:29:32.056526Z",
     "iopub.status.idle": "2025-10-15T07:29:32.076974Z",
     "shell.execute_reply": "2025-10-15T07:29:32.076259Z",
     "shell.execute_reply.started": "2025-10-15T07:29:32.056754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "class Wav2Vec2ID(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_classes=900, freeze_encoder=True):\n",
    "        super().__init__()\n",
    "        from transformers import Wav2Vec2Model\n",
    "        self.encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        if freeze_encoder:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.fc_hidden = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x).last_hidden_state   # [B, T', H]\n",
    "        pooled = out.mean(dim=1)\n",
    "        emb = self.fc_hidden(pooled)\n",
    "        logits = self.fc_out(emb)\n",
    "        return emb, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T07:29:32.077969Z",
     "iopub.status.busy": "2025-10-15T07:29:32.077769Z",
     "iopub.status.idle": "2025-10-15T07:29:32.100422Z",
     "shell.execute_reply": "2025-10-15T07:29:32.099649Z",
     "shell.execute_reply.started": "2025-10-15T07:29:32.077954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AAMSoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, emb_dim=256, margin=0.3, scale=30):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        self.weight = nn.Parameter(torch.randn(num_classes, emb_dim))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "    def forward(self, emb, labels):\n",
    "        emb = F.normalize(emb)\n",
    "        W = F.normalize(self.weight)\n",
    "\n",
    "        cosine = F.linear(emb, W)  # [B, num_classes]\n",
    "\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logit = torch.cos(theta + self.margin)\n",
    "\n",
    "        one_hot = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "        logits = cosine * (1 - one_hot) + target_logit * one_hot\n",
    "\n",
    "        logits *= self.scale\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "def train_one_epoch_am(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for x, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emb, _ = model(x)\n",
    "        loss, logits = criterion(emb, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def validate_one_epoch_am(model, loader, device, normalize_emb=True):\n",
    "    model.eval()\n",
    "    all_scores, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor, pair, label in tqdm(loader, desc=\"Valid\", leave=False):\n",
    "            anchor = anchor.to(device)\n",
    "            pair = pair.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            emb1, _ = model(anchor)\n",
    "            emb2, _ = model(pair)\n",
    "\n",
    "            if normalize_emb:\n",
    "                emb1 = F.normalize(emb1, dim=1)\n",
    "                emb2 = F.normalize(emb2, dim=1)\n",
    "\n",
    "            scores = F.cosine_similarity(emb1, emb2)\n",
    "            all_scores.append(scores.cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "\n",
    "    all_scores = torch.cat(all_scores).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2.0\n",
    "    return eer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_s = 64\n",
    "\n",
    "tra_ds = SpeakerClassificationFeatureDataset(root, all_files, fixed_len=40000, one_hot=False, augment=True)\n",
    "tra_dl = DataLoader(tra_ds, batch_size=batch_s, shuffle=True, collate_fn=collate_fn_classification)\n",
    "tes_ds = SiameseSpeakerFeatureDataset(root_test, o_files, fixed_len=40000)\n",
    "tes_dl = DataLoader(tes_ds, batch_size=batch_s, shuffle=True, collate_fn=siamese_collate_fn)\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T07:59:02.516332Z",
     "iopub.status.busy": "2025-10-14T07:59:02.516031Z",
     "iopub.status.idle": "2025-10-14T07:59:06.045981Z",
     "shell.execute_reply": "2025-10-14T07:59:06.045431Z",
     "shell.execute_reply.started": "2025-10-14T07:59:02.516303Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Wav2Vec2ID(num_classes=879, hidden_size=512, freeze_encoder=True).to(device)\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_am = AAMSoftmaxLoss(num_classes=879, emb_dim=512, margin=0.2, scale=15).to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(criterion_am.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T07:59:14.712815Z",
     "iopub.status.busy": "2025-10-14T07:59:14.712106Z",
     "iopub.status.idle": "2025-10-14T17:14:14.133876Z",
     "shell.execute_reply": "2025-10-14T17:14:14.132115Z",
     "shell.execute_reply.started": "2025-10-14T07:59:14.712792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, 25):\n",
    "\n",
    "    if epoch == 2:\n",
    "        for p in model.encoder.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    if epoch == 0:\n",
    "        tr_loss, tr_acc = train_one_epoch_am(model, tra_dl, criterion_ce, optimizer, device)\n",
    "        eer = validate_one_epoch_am(model, tes_dl, device)\n",
    "    else:\n",
    "        tr_loss, tr_acc = train_one_epoch_am(model, tra_dl, criterion_am, optimizer, device)\n",
    "        eer = validate_one_epoch_am(model, tes_dl, device)\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={tr_loss:.4f}, Train Acc={tr_acc:.4f}, EER={eer*100:.2f}%\")\n",
    "\n",
    "    optimizer_info = {\n",
    "        'param_groups': [\n",
    "            {k: v for k, v in group.items() if k in ['lr', 'betas', 'weight_decay']}\n",
    "            for group in optimizer.param_groups\n",
    "        ]\n",
    "    }\n",
    "    ckpt_path = f\"wav2vec2_vvn_{epoch}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"criterion_state\": criterion_am.state_dict(),\n",
    "        \"optimizer_info\": optimizer_info\n",
    "    }, ckpt_path)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7251391,
     "sourceId": 11565412,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8345362,
     "sourceId": 13169831,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
