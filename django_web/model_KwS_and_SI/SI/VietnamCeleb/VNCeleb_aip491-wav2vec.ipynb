{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-16T05:23:35.393762Z",
     "iopub.status.busy": "2025-10-16T05:23:35.393517Z",
     "iopub.status.idle": "2025-10-16T05:23:55.390779Z",
     "shell.execute_reply": "2025-10-16T05:23:55.389970Z",
     "shell.execute_reply.started": "2025-10-16T05:23:35.393741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers huggingface_hub httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "root = \"/kaggle/input/vietnam-celeb-dataset/full-dataset/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:24:03.950316Z",
     "iopub.status.busy": "2025-10-16T05:24:03.949760Z",
     "iopub.status.idle": "2025-10-16T05:24:18.589766Z",
     "shell.execute_reply": "2025-10-16T05:24:18.588805Z",
     "shell.execute_reply.started": "2025-10-16T05:24:03.950297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_all_wavs(root, except_path):\n",
    "    all_wavs = []\n",
    "    spk2label = {}\n",
    "    for spk_folder in sorted(os.listdir(root)):\n",
    "        fpath = os.path.join(root, spk_folder)\n",
    "        if not os.path.isdir(fpath):\n",
    "            continue\n",
    "        for w in sorted(os.listdir(fpath)):\n",
    "            if w.endswith(\".wav\"):\n",
    "                rel_path = f\"{spk_folder}/{w}\"\n",
    "                if rel_path in except_path:\n",
    "                    continue\n",
    "                all_wavs.append(rel_path)\n",
    "    return all_wavs\n",
    "\n",
    "\n",
    "def read_txt_paths(txt_file, valid_paths):\n",
    "    data = []\n",
    "    els_path = []\n",
    "    num_removed = 0\n",
    "\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) != 3:\n",
    "                raise ValueError(f\"{line}\")\n",
    "            label, path1, path2 = parts\n",
    "\n",
    "            if path1 not in valid_paths or path2 not in valid_paths:\n",
    "                num_removed += 1\n",
    "                continue\n",
    "\n",
    "            data.append((int(label), path1, path2))\n",
    "            els_path.append(path1)\n",
    "            els_path.append(path2)\n",
    "\n",
    "    return data, num_removed, set(els_path)\n",
    "\n",
    "\n",
    "a_files = set(list_all_wavs(root, []))\n",
    "print(len(a_files))\n",
    "\n",
    "e_files, e_r, e_e = read_txt_paths(\"/kaggle/input/vietnam-celeb-dataset/full-dataset/vietnam-celeb-e.txt\", set(a_files))\n",
    "print(len(e_files), e_r)\n",
    "\n",
    "h_files, h_r, h_e = read_txt_paths(\"/kaggle/input/vietnam-celeb-dataset/full-dataset/vietnam-celeb-h.txt\", set(a_files))\n",
    "print(len(h_files), h_r)\n",
    "\n",
    "print(len(a_files - (e_e | h_e)))\n",
    "\n",
    "print(e_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:24:18.591954Z",
     "iopub.status.busy": "2025-10-16T05:24:18.591714Z",
     "iopub.status.idle": "2025-10-16T05:24:20.000591Z",
     "shell.execute_reply": "2025-10-16T05:24:19.999407Z",
     "shell.execute_reply.started": "2025-10-16T05:24:18.591934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def assign_labels_from_ids(file_list):\n",
    "    data = []\n",
    "    spk2label = {}\n",
    "\n",
    "    for path in file_list:\n",
    "        spk = path.split(\"/\")[0]\n",
    "\n",
    "        if spk not in spk2label:\n",
    "            spk2label[spk] = len(spk2label)\n",
    "\n",
    "        label = spk2label[spk]\n",
    "        data.append((label, path))\n",
    "\n",
    "    return data, spk2label\n",
    "\n",
    "all_files, all_spk = assign_labels_from_ids(list_all_wavs(root, e_e|h_e))\n",
    "print(len(all_files), len(all_spk))\n",
    "print(all_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:24:20.002030Z",
     "iopub.status.busy": "2025-10-16T05:24:20.001710Z",
     "iopub.status.idle": "2025-10-16T05:24:20.031650Z",
     "shell.execute_reply": "2025-10-16T05:24:20.030629Z",
     "shell.execute_reply.started": "2025-10-16T05:24:20.002000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SpeakerClassificationFeatureDataset(Dataset):\n",
    "    def __init__(self, root, file_list, fixed_len=64000, one_hot=True,\n",
    "                 augment=False, sample_rate=16000, si=16):\n",
    "        \n",
    "        self.root = root\n",
    "        self.fixed_len = fixed_len\n",
    "        self.one_hot = one_hot\n",
    "        self.augment = augment\n",
    "        self.sample_rate = sample_rate\n",
    "        self.si = si\n",
    "\n",
    "        self.data = [(label, os.path.join(root, path)) for label, path in file_list]\n",
    "        self.num_speakers = max(label for label, _ in file_list) + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, path = self.data[idx]\n",
    "\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        wav = wav.mean(dim=0)\n",
    "\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            wav = resampler(wav.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                wav = wav * random.uniform(0.9, 1.1)\n",
    "            if random.random() < 0.5:\n",
    "                wav = wav + torch.randn_like(wav) * 0.005\n",
    "\n",
    "        wav = self._fix_length(wav)\n",
    "\n",
    "        if self.one_hot:\n",
    "            lbl = torch.zeros(self.num_speakers)\n",
    "            lbl[label] = 1.0\n",
    "        else:\n",
    "            lbl = label\n",
    "\n",
    "        return wav, lbl\n",
    "\n",
    "    def _fix_length(self, wav):\n",
    "        L = wav.size(0)\n",
    "        if L > self.fixed_len:\n",
    "            start = (L - self.fixed_len) // 2\n",
    "            return wav[start:start + self.fixed_len]\n",
    "        elif L < self.fixed_len:\n",
    "            return F.pad(wav, (0, self.fixed_len - L))\n",
    "        return wav\n",
    "\n",
    "\n",
    "class SiameseSpeakerFeatureDataset(Dataset):\n",
    "    def __init__(self, root, file_list, fixed_len=64000, sample_rate=16000, si=16):\n",
    "        self.root = root\n",
    "        self.file_list = file_list\n",
    "        self.fixed_len = fixed_len\n",
    "        self.sample_rate = sample_rate\n",
    "        self.si = si\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, rel1, rel2 = self.file_list[idx]\n",
    "        p1 = os.path.join(self.root, rel1)\n",
    "        p2 = os.path.join(self.root, rel2)\n",
    "\n",
    "        wav1, sr1 = torchaudio.load(p1)\n",
    "        wav2, sr2 = torchaudio.load(p2)\n",
    "        wav1 = wav1.mean(dim=0)\n",
    "        wav2 = wav2.mean(dim=0)\n",
    "\n",
    "        if sr1 != self.sample_rate:\n",
    "            wav1 = torchaudio.transforms.Resample(sr1, self.sample_rate)(wav1.unsqueeze(0)).squeeze(0)\n",
    "        if sr2 != self.sample_rate:\n",
    "            wav2 = torchaudio.transforms.Resample(sr2, self.sample_rate)(wav2.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        wav1 = self._fix_length(wav1)\n",
    "        wav2 = self._fix_length(wav2)\n",
    "\n",
    "        return wav1, wav2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def _fix_length(self, wav):\n",
    "        L = wav.size(0)\n",
    "        if L > self.fixed_len:\n",
    "            start = (L - self.fixed_len) // 2\n",
    "            return wav[start:start + self.fixed_len]\n",
    "        elif L < self.fixed_len:\n",
    "            return F.pad(wav, (0, self.fixed_len - L))\n",
    "        return wav\n",
    "\n",
    "def siamese_collate_fn(batch):\n",
    "    anchors, pairs, labels = zip(*batch)\n",
    "    anchors = torch.stack(anchors)\n",
    "    pairs = torch.stack(pairs)\n",
    "    labels = torch.tensor(labels).float()\n",
    "    return anchors, pairs, labels\n",
    "\n",
    "def collate_fn_classification(batch):\n",
    "    wavs = torch.stack([b[0] for b in batch], dim=0)\n",
    "    labels = torch.tensor([b[1] for b in batch], dtype=torch.long)\n",
    "    return wavs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:24:20.033439Z",
     "iopub.status.busy": "2025-10-16T05:24:20.032527Z",
     "iopub.status.idle": "2025-10-16T05:24:21.034333Z",
     "shell.execute_reply": "2025-10-16T05:24:21.033468Z",
     "shell.execute_reply.started": "2025-10-16T05:24:20.033409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "class Wav2Vec2ID(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, num_classes=900, freeze_encoder=True):\n",
    "        super().__init__()\n",
    "        from transformers import Wav2Vec2Model\n",
    "        self.encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "        if freeze_encoder:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.fc_hidden = nn.Sequential(\n",
    "            nn.Linear(self.encoder.config.hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x).last_hidden_state\n",
    "        pooled = out.mean(dim=1)\n",
    "        emb = self.fc_hidden(pooled)\n",
    "        logits = self.fc_out(emb)\n",
    "        return emb, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:24:21.036135Z",
     "iopub.status.busy": "2025-10-16T05:24:21.035291Z",
     "iopub.status.idle": "2025-10-16T05:24:21.056312Z",
     "shell.execute_reply": "2025-10-16T05:24:21.055459Z",
     "shell.execute_reply.started": "2025-10-16T05:24:21.036104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AAMSoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, emb_dim=256, margin=0.3, scale=30):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        self.weight = nn.Parameter(torch.randn(num_classes, emb_dim))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "    def forward(self, emb, labels):\n",
    "        emb = F.normalize(emb)\n",
    "        W = F.normalize(self.weight)\n",
    "\n",
    "        cosine = F.linear(emb, W)  # [B, num_classes]\n",
    "\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logit = torch.cos(theta + self.margin)\n",
    "\n",
    "        one_hot = F.one_hot(labels, num_classes=self.num_classes).float()\n",
    "        logits = cosine * (1 - one_hot) + target_logit * one_hot\n",
    "\n",
    "        logits *= self.scale\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss, logits\n",
    "\n",
    "def train_one_epoch_am(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for x, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emb, _ = model(x)\n",
    "        loss, logits = criterion(emb, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def validate_one_epoch_am(model, loader, device, normalize_emb=True):\n",
    "    model.eval()\n",
    "    all_scores, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor, pair, label in tqdm(loader, desc=\"Valid\", leave=False):\n",
    "            anchor = anchor.to(device)\n",
    "            pair = pair.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            emb1, _ = model(anchor)\n",
    "            emb2, _ = model(pair)\n",
    "\n",
    "            if normalize_emb:\n",
    "                emb1 = F.normalize(emb1, dim=1)\n",
    "                emb2 = F.normalize(emb2, dim=1)\n",
    "\n",
    "            scores = F.cosine_similarity(emb1, emb2)\n",
    "            all_scores.append(scores.cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "\n",
    "    all_scores = torch.cat(all_scores).numpy()\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2.0\n",
    "    return eer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:24:21.057738Z",
     "iopub.status.busy": "2025-10-16T05:24:21.057161Z",
     "iopub.status.idle": "2025-10-16T05:24:21.244235Z",
     "shell.execute_reply": "2025-10-16T05:24:21.243402Z",
     "shell.execute_reply.started": "2025-10-16T05:24:21.057718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_s = 64\n",
    "\n",
    "tra_ds = SpeakerClassificationFeatureDataset(root, all_files, fixed_len=40000, one_hot=False, augment=True)\n",
    "tra_dl = DataLoader(tra_ds, batch_size=batch_s, shuffle=True, collate_fn=collate_fn_classification)\n",
    "et_ds = SiameseSpeakerFeatureDataset(root, e_files, fixed_len=40000)\n",
    "et_dl = DataLoader(et_ds, batch_size=batch_s, shuffle=True, collate_fn=siamese_collate_fn)\n",
    "ht_ds = SiameseSpeakerFeatureDataset(root, h_files, fixed_len=40000)\n",
    "ht_dl = DataLoader(ht_ds, batch_size=batch_s, shuffle=True, collate_fn=siamese_collate_fn)\n",
    "\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:23:02.082528Z",
     "iopub.status.busy": "2025-10-15T10:23:02.081937Z",
     "iopub.status.idle": "2025-10-15T10:23:22.653079Z",
     "shell.execute_reply": "2025-10-15T10:23:22.652501Z",
     "shell.execute_reply.started": "2025-10-15T10:23:02.082505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Wav2Vec2ID(num_classes=905, hidden_size=512, freeze_encoder=True).to(device)\n",
    "criterion = AAMSoftmaxLoss(num_classes=905, emb_dim=512, margin=0.025, scale=2).to(device)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + list(criterion.parameters()), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-16T05:11:53.970Z",
     "iopub.execute_input": "2025-10-16T01:12:49.624818Z",
     "iopub.status.busy": "2025-10-16T01:12:49.623776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, 25):\n",
    "\n",
    "    criterion.margin = (epoch + 1)*0.025\n",
    "    criterion.scale = (epoch + 1)*2\n",
    "\n",
    "    tr_loss, tr_acc = train_one_epoch_am(model, tra_dl, criterion, optimizer, device)\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1)%2 == 0:\n",
    "        e_eer = validate_one_epoch_am(model, et_dl, device)\n",
    "        h_eer = validate_one_epoch_am(model, ht_dl, device)\n",
    "    else:\n",
    "        e_eer = 0\n",
    "        h_eer = 0\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss={tr_loss:.4f}, Train Acc={tr_acc:.4f}, E EER={e_eer*100:.2f}%, H EER={h_eer*100:.2f}%\")\n",
    "\n",
    "    optimizer_info = {\n",
    "        'param_groups': [\n",
    "            {k: v for k, v in group.items() if k in ['lr', 'betas', 'weight_decay']}\n",
    "            for group in optimizer.param_groups\n",
    "        ]\n",
    "    }\n",
    "    ckpt_path = f\"wav2vec2_vnc_{epoch}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"criterion_state\": criterion.state_dict(),\n",
    "        \"optimizer_info\": optimizer_info\n",
    "    }, ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6096635,
     "sourceId": 9920101,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 473809,
     "modelInstanceId": 457901,
     "sourceId": 609801,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 473809,
     "modelInstanceId": 457901,
     "sourceId": 610138,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
