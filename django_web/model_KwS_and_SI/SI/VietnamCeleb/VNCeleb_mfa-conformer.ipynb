{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4687d431",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T18:05:30.410825Z",
     "iopub.status.busy": "2025-11-09T18:05:30.410615Z",
     "iopub.status.idle": "2025-11-09T22:42:21.090328Z",
     "shell.execute_reply": "2025-11-09T22:42:21.089565Z"
    },
    "papermill": {
     "duration": 16610.684405,
     "end_time": "2025-11-09T22:42:21.091746",
     "exception": false,
     "start_time": "2025-11-09T18:05:30.407341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, glob, random, gc, shutil\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\" DEVICE:\", DEVICE)\n",
    "\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/vietnam-celeb-dataset/full-dataset\"\n",
    "AUDIO_DIR = os.path.join(DATA_ROOT, \"data\")\n",
    "TXT_E = os.path.join(DATA_ROOT, \"vietnam-celeb-e.txt\")\n",
    "TXT_H = os.path.join(DATA_ROOT, \"vietnam-celeb-h.txt\")\n",
    "\n",
    "EXTS = (\".wav\", \".flac\", \".mp3\", \".m4a\", \".ogg\")\n",
    "N_MELS = 80   \n",
    "\n",
    "def _norm_rel(p: str) -> str:\n",
    "    p = p.strip().replace(\"\\\\\", \"/\").lstrip(\"./\")\n",
    "    if p.startswith(\"data/\"): p = p.split(\"data/\", 1)[1]\n",
    "    return p\n",
    "\n",
    "def read_pairs(txt_path):\n",
    "    pairs = []\n",
    "    if not os.path.exists(txt_path): return pairs\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            parts = ln.strip().split()\n",
    "            if len(parts) >= 3:\n",
    "                try:\n",
    "                    lab = int(parts[0])\n",
    "                    a_rel = _norm_rel(parts[1])\n",
    "                    b_rel = _norm_rel(parts[2])\n",
    "                    pairs.append((lab, a_rel, b_rel))\n",
    "                except: continue\n",
    "    return pairs\n",
    "\n",
    "E_pairs = read_pairs(TXT_E)\n",
    "H_pairs = read_pairs(TXT_H)\n",
    "E_files = {p for _, a, b in E_pairs for p in [a,b]}\n",
    "H_files = {p for _, a, b in H_pairs for p in [a,b]}\n",
    "ban = E_files | H_files\n",
    "\n",
    "all_abs = [p for p in glob.glob(os.path.join(AUDIO_DIR, \"**\", \"*\"), recursive=True)\n",
    "           if os.path.splitext(p)[1].lower() in EXTS]\n",
    "kept_abs = [p for p in all_abs\n",
    "            if os.path.relpath(p, AUDIO_DIR).replace(\"\\\\\", \"/\") not in ban]\n",
    "print(f\" Total audio: {len(all_abs)} | Train usable: {len(kept_abs)}\")\n",
    "\n",
    "\n",
    "spk2files_all = defaultdict(list)\n",
    "for p in kept_abs:\n",
    "    sid = os.path.basename(os.path.dirname(p))\n",
    "    spk2files_all[sid].append(p)\n",
    "spk2files = {s: fs for s, fs in spk2files_all.items() if len(fs) >= 10}\n",
    "speakers = sorted(spk2files.keys())\n",
    "spk2idx = {s: i for i, s in enumerate(speakers)}\n",
    "num_classes = len(spk2idx)\n",
    "print(f\" Speakers for training: {num_classes}\")\n",
    "\n",
    "\n",
    "train_pairs = []\n",
    "for sid, files in spk2files.items():\n",
    "    random.shuffle(files)\n",
    "    n = len(files)\n",
    "    n_tr = int(0.9 * n)\n",
    "    train_pairs += [(p, spk2idx[sid]) for p in files[:n_tr]]\n",
    "print(f\"Train samples: {len(train_pairs)}\")\n",
    "\n",
    "\n",
    "MEL_TRANSFORM = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=16000, n_fft=400, hop_length=160, n_mels=N_MELS\n",
    ")\n",
    "\n",
    "def is_valid_audio(path):\n",
    "    try:\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if wav.shape[0] > 1: wav = wav.mean(0, keepdim=True)\n",
    "        if sr != 16000: wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "        return wav.abs().mean().item() > 1e-4\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "train_pairs = [(p, y) for p, y in train_pairs if is_valid_audio(p)]\n",
    "print(f\" After cleaning: {len(train_pairs)} valid samples\")\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, pairs, augment=False, sr=16000, max_len=3.0):\n",
    "        self.pairs = pairs; self.augment = augment\n",
    "        self.sr = sr; self.max_len = max_len\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        path, y = self.pairs[idx]\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if wav.shape[0] > 1: wav = wav.mean(0, keepdim=True)\n",
    "        if sr != self.sr: wav = torchaudio.functional.resample(wav, sr, self.sr)\n",
    "        L = int(self.max_len * self.sr)\n",
    "        if wav.size(1) > L:\n",
    "            st = random.randint(0, wav.size(1)-L); wav = wav[:, st:st+L]\n",
    "        else:\n",
    "            wav = F.pad(wav, (0, L - wav.size(1)))\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5: wav += 0.005 * torch.randn_like(wav)\n",
    "            if random.random() < 0.5: wav *= random.uniform(0.8, 1.2)\n",
    "        mel = torch.log(MEL_TRANSFORM(wav) + 1e-6).squeeze(0)\n",
    "        mel = torch.nan_to_num(mel)\n",
    "        return mel, y\n",
    "\n",
    "BATCH = 16\n",
    "dl_tr = DataLoader(VoiceDataset(train_pairs, True), batch_size=BATCH, shuffle=True, num_workers=2)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x): return x * torch.sigmoid(x)\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, dim, k=15):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.pw1 = nn.Conv1d(dim, 2*dim, 1)\n",
    "        self.dw  = nn.Conv1d(dim, dim, k, padding=k//2, groups=dim)\n",
    "        self.bn  = nn.BatchNorm1d(dim)\n",
    "        self.act = Swish()\n",
    "        self.pw2 = nn.Conv1d(dim, dim, 1)\n",
    "    def forward(self, x):\n",
    "        y = self.ln(x).transpose(1,2)\n",
    "        y = F.glu(self.pw1(y), dim=1)\n",
    "        y = self.pw2(self.act(self.bn(self.dw(y)))).transpose(1,2)\n",
    "        return x + y\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, dim, exp=8, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, exp*dim), Swish(), nn.Dropout(drop),\n",
    "            nn.Linear(exp*dim, dim), nn.Dropout(drop)\n",
    "        )\n",
    "    def forward(self, x): return x + 0.5 * self.ff(self.ln(x))\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, h=4, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, h, dropout=drop, batch_first=True)\n",
    "        self.do = nn.Dropout(drop)\n",
    "    def forward(self, x):\n",
    "        y, _ = self.attn(self.ln(x), self.ln(x), self.ln(x))\n",
    "        return x + self.do(y)\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, dim, h=4, k=15, exp=8, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.ff1 = FeedForwardModule(dim, exp, drop)\n",
    "        self.mhsa = MHSA(dim, h, drop)\n",
    "        self.conv = ConvModule(dim, k)\n",
    "        self.ff2 = FeedForwardModule(dim, exp, drop)\n",
    "    def forward(self, x): return self.ff2(self.conv(self.mhsa(self.ff1(x))))\n",
    "\n",
    "class MFAConformer(nn.Module):\n",
    "    def __init__(self, n_mels=N_MELS, dim=256, L=6, h=4, k=15, exp=8, drop=0.2, emb_dim=192):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(n_mels, dim)\n",
    "        self.blocks = nn.ModuleList([ConformerBlock(dim, h, k, exp, drop) for _ in range(L)])\n",
    "        self.ln_mfa = nn.LayerNorm(dim * L)\n",
    "        self.post = nn.Sequential(nn.Linear(dim * L * 2, emb_dim), nn.BatchNorm1d(emb_dim))\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x.transpose(1, 2))\n",
    "        feats = []\n",
    "        for b in self.blocks:\n",
    "            x = b(x); feats.append(x)\n",
    "        H = self.ln_mfa(torch.cat(feats, dim=-1))\n",
    "        mean, std = H.mean(1), H.std(1).clamp(min=1e-6)\n",
    "        emb = self.post(torch.cat([mean, std], 1))\n",
    "        emb = torch.nan_to_num(emb)\n",
    "        return F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "def compute_eer(scores, labels):\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    fnr = 1 - tpr\n",
    "    idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "    return float((fpr[idx] + fnr[idx]) / 2 * 100)\n",
    "\n",
    "def safe_cosine(a, b):\n",
    "    a = F.normalize(torch.nan_to_num(a), p=2, dim=0)\n",
    "    b = F.normalize(torch.nan_to_num(b), p=2, dim=0)\n",
    "    s = F.cosine_similarity(a, b, dim=0).item()\n",
    "    return None if not np.isfinite(s) else float(s)\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_file(p_abs, model):\n",
    "    try:\n",
    "        wav, sr = torchaudio.load(p_abs)\n",
    "        if wav.shape[0] > 1: wav = wav.mean(0, keepdim=True)\n",
    "        if sr != 16000: wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "        wav = F.pad(wav, (0, max(0, 48000 - wav.size(1))))[:, :48000]\n",
    "        mel = torch.log(MEL_TRANSFORM(wav)+1e-6).squeeze(0)\n",
    "        mel = torch.nan_to_num(mel)\n",
    "        with torch.amp.autocast(\"cuda\", enabled=False):\n",
    "            emb = model(mel.unsqueeze(0).to(DEVICE, dtype=torch.float32)).cpu().squeeze(0)\n",
    "        return torch.nan_to_num(emb)\n",
    "    except: return None\n",
    "\n",
    "def eval_eer(pairs, model):\n",
    "    needed = sorted({os.path.join(AUDIO_DIR, a) for _, a, _ in pairs} |\n",
    "                    {os.path.join(AUDIO_DIR, b) for _, _, b in pairs})\n",
    "    emb_map = {}\n",
    "    for p in needed:\n",
    "        e = embed_file(p, model)\n",
    "        if e is not None: emb_map[os.path.relpath(p, AUDIO_DIR)] = e\n",
    "    scores, labels = [], []\n",
    "    for lab, a, b in pairs:\n",
    "        if a not in emb_map or b not in emb_map: continue\n",
    "        s = safe_cosine(emb_map[a], emb_map[b])\n",
    "        if s is not None:\n",
    "            scores.append(s); labels.append(lab)\n",
    "    if len(scores) < 5: return None\n",
    "    return compute_eer(scores, labels)\n",
    "\n",
    "model = MFAConformer(n_mels=N_MELS).to(DEVICE)\n",
    "head = nn.Linear(192, num_classes).to(DEVICE)\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(head.parameters()), lr=1e-4, weight_decay=1e-5)\n",
    "crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "scaler = GradScaler()\n",
    "\n",
    "EPOCHS = 30\n",
    "steps = len(dl_tr)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=1e-4, epochs=EPOCHS, steps_per_epoch=steps)\n",
    "best_eer = 999.0\n",
    "\n",
    "os.makedirs(\"/kaggle/working/mfa_conf_paper\", exist_ok=True)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train(); head.train()\n",
    "    total, correct = 0, 0\n",
    "    for mel, y in dl_tr:\n",
    "        mel, y = mel.to(DEVICE), y.to(DEVICE)\n",
    "        with autocast(\"cuda\", dtype=torch.float16):\n",
    "            emb = model(mel)\n",
    "            logits = head(emb)\n",
    "            loss = crit(logits, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt); scaler.update()\n",
    "        sched.step()\n",
    "        total += y.size(0)\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "    acc = correct / total\n",
    "    print(f\"[Ep{ep:02d}] train_acc={acc:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    eer_e = eval_eer(E_pairs, model)\n",
    "    eer_h = eval_eer(H_pairs, model)\n",
    "    print(f\"  ↳ Eval EER_E={eer_e if eer_e else 'N/A'} | EER_H={eer_h if eer_h else 'N/A'}\")\n",
    "    if eer_e and eer_e < best_eer:\n",
    "        best_eer = eer_e\n",
    "        torch.save({\"model\": model.state_dict(), \"head\": head.state_dict(),\n",
    "                    \"EER_E\": eer_e, \"EER_H\": eer_h},\n",
    "                   \"/kaggle/working/mfa_conf_paper/best.pt\")\n",
    "        print(\" Saved new best model\")\n",
    "\n",
    "gc.collect()\n",
    "if DEVICE == \"cuda\": torch.cuda.empty_cache()\n",
    "shutil.make_archive(\"/kaggle/working/mfa_conf_paper_export\", \"zip\", \"/kaggle/working/mfa_conf_paper\")\n",
    "print(f\" Done. Best EER_E ≈ {best_eer:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6096635,
     "sourceId": 9920101,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16619.374897,
   "end_time": "2025-11-09T22:42:24.601766",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-09T18:05:25.226869",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
