{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ee4bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T03:12:16.605007Z",
     "iopub.status.busy": "2025-11-12T03:12:16.604746Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-11-12T03:12:16.599728",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, glob, random, json, math, gc, time, shutil, itertools\n",
    "import numpy as np\n",
    "import torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_curve, accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchaudio import pipelines as TAP\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DATA_ROOT = \"/kaggle/input/vietnam-celeb-dataset/full-dataset\"\n",
    "AUDIO_DIR = os.path.join(DATA_ROOT, \"data\")\n",
    "TXT_E     = os.path.join(DATA_ROOT, \"vietnam-celeb-e.txt\")\n",
    "TXT_H     = os.path.join(DATA_ROOT, \"vietnam-celeb-h.txt\")\n",
    "assert os.path.exists(AUDIO_DIR), f\"Không thấy thư mục audio: {AUDIO_DIR}\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"AUDIO_DIR:\", AUDIO_DIR)\n",
    "\n",
    "FAST_MODE           = False        \n",
    "PROFILE             = \"M\"          \n",
    "USE_WAVLM_LARGE     = False        \n",
    "\n",
    "CFG = {\n",
    "  \"S\":  dict(d_model=256, nhead=4, d_ff=1024, n_layers=6,  emb_dim=192, k=31, bs=14, steps=6000, max_seconds=2.0),\n",
    "  \"M\":  dict(d_model=320, nhead=5, d_ff=1280, n_layers=8,  emb_dim=192, k=31, bs=12, steps=6000, max_seconds=2.0),\n",
    "  \"L\":  dict(d_model=384, nhead=6, d_ff=1536, n_layers=10, emb_dim=256, k=41, bs=10, steps=5500, max_seconds=2.0),\n",
    "  \"XL\": dict(d_model=512, nhead=8, d_ff=2048, n_layers=12, emb_dim=256, k=41, bs=6,  steps=5000, max_seconds=1.6),\n",
    "}\n",
    "P = CFG[PROFILE]\n",
    "\n",
    "TARGET_SR           = 16000\n",
    "MAX_SECONDS         = P[\"max_seconds\"]\n",
    "MAX_SECONDS_VERIFY  = 4.0\n",
    "BATCH_SIZE          = P[\"bs\"]\n",
    "NUM_WORKERS         = 2\n",
    "MIN_UTTS_PER_SPK    = 5\n",
    "SPLIT_RATIO         = (0.80, 0.10, 0.10)\n",
    "\n",
    "\n",
    "E_LP = 3       \n",
    "E_A  = 4       \n",
    "E_B  = 30      \n",
    "STEPS_PER_EPOCH_TRAIN = P[\"steps\"]\n",
    "\n",
    "\n",
    "EER_EVAL_EVERY_LP = 1\n",
    "EER_EVAL_EVERY_FT = 1\n",
    "MAX_PAIRS_LP      = 2500 if not FAST_MODE else 1500\n",
    "MAX_PAIRS_FT      = 5000 if not FAST_MODE else 3000\n",
    "EER_CROP_MODE     = \"tta\"     \n",
    "USE_SNORM         = True\n",
    "\n",
    "\n",
    "COHORT_K_INIT  = 6000 if not FAST_MODE else 2000\n",
    "COHORT_K_FT    = 10000 if not FAST_MODE else 4000\n",
    "SNORM_TOPM     = 600\n",
    "EARLY_STOP_PATIENCE = 2\n",
    "\n",
    "print(\"HYPERPARAMS:\", dict(TARGET_SR=TARGET_SR, BATCH_SIZE=BATCH_SIZE, FAST_MODE=FAST_MODE))\n",
    "\n",
    "\n",
    "def _norm_rel(p: str) -> str:\n",
    "    p = p.strip().replace(\"\\\\\", \"/\").lstrip(\"./\")\n",
    "    if p.startswith(\"data/\"): p = p.split(\"data/\", 1)[1]\n",
    "    return p\n",
    "\n",
    "def read_pairs_to_set(txt_path: str) -> set:\n",
    "    s = set()\n",
    "    if not os.path.exists(txt_path): return s\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln or ln.startswith(\"#\"): continue\n",
    "            parts = ln.split()\n",
    "            if len(parts) >= 3:\n",
    "                s.add(_norm_rel(parts[1])); s.add(_norm_rel(parts[2]))\n",
    "    return s\n",
    "\n",
    "E_files = read_pairs_to_set(TXT_E)\n",
    "H_files = read_pairs_to_set(TXT_H)\n",
    "ban = E_files | H_files\n",
    "print(\"Files in E:\", len(E_files), \"| Files in H:\", len(H_files), \"| BAN total:\", len(ban))\n",
    "\n",
    "EXTS = (\".wav\", \".flac\", \".mp3\", \".m4a\", \".ogg\")\n",
    "all_abs = [p for p in glob.glob(os.path.join(AUDIO_DIR, \"**\", \"*\"), recursive=True)\n",
    "           if os.path.splitext(p)[1].lower() in EXTS]\n",
    "print(\"Found audio in data/:\", len(all_abs))\n",
    "\n",
    "kept_abs = [p for p in all_abs\n",
    "            if os.path.relpath(p, AUDIO_DIR).replace(\"\\\\\", \"/\") not in ban]\n",
    "print(\"Kept for training (data \\\\ (E ∪ H)):\", len(kept_abs))\n",
    "assert len(kept_abs) > 0\n",
    "\n",
    "spk2files_all = defaultdict(list)\n",
    "for p in kept_abs:\n",
    "    sid = os.path.basename(os.path.dirname(p))\n",
    "    spk2files_all[sid].append(p)\n",
    "\n",
    "spk2files = {s: fs for s, fs in spk2files_all.items() if len(fs) >= MIN_UTTS_PER_SPK}\n",
    "speakers = sorted(spk2files.keys())\n",
    "spk2idx = {s: i for i, s in enumerate(speakers)}\n",
    "idx2spk = {i: s for s, i in spk2idx.items()}\n",
    "num_classes = len(speakers)\n",
    "print(\"Speakers kept:\", num_classes)\n",
    "assert num_classes >= 2\n",
    "\n",
    "SPLIT_TR, SPLIT_VA, _ = SPLIT_RATIO\n",
    "train_pairs, val_pairs, test_pairs = [], [], []\n",
    "random.seed(123)\n",
    "for sid, files in spk2files.items():\n",
    "    files = files[:] ; random.shuffle(files)\n",
    "    n = len(files)\n",
    "    n_tr = max(3, int(SPLIT_TR * n))\n",
    "    n_va = max(1, int(SPLIT_VA * n))\n",
    "    tr = files[:n_tr]; va = files[n_tr:n_tr + n_va]; te = files[n_tr + n_va:]\n",
    "    if len(te) == 0 and len(va) > 0: te = [va.pop()]\n",
    "    if len(te) == 0 and len(tr) > 1: te = [tr.pop()]\n",
    "    train_pairs += [(p, spk2idx[sid]) for p in tr]\n",
    "    val_pairs   += [(p, spk2idx[sid]) for p in va]\n",
    "    test_pairs  += [(p, spk2idx[sid]) for p in te]\n",
    "\n",
    "print(\"Split sizes (train/val/test):\", len(train_pairs), len(val_pairs), len(test_pairs))\n",
    "to_rel = lambda p: os.path.relpath(p, AUDIO_DIR).replace(\"\\\\\", \"/\")\n",
    "train_rel = {to_rel(p) for p, _ in train_pairs}\n",
    "val_rel   = {to_rel(p) for p, _ in val_pairs}\n",
    "test_rel  = {to_rel(p) for p, _ in test_pairs}\n",
    "print(\"train ∩ E:\", len(train_rel & E_files), \"| train ∩ H:\", len(train_rel & H_files))\n",
    "print(\" val  ∩ E:\", len(val_rel   & E_files), \"|  val  ∩ H:\", len(val_rel   & H_files))\n",
    "print(\" test ∩ E:\", len(test_rel  & E_files), \"| test  ∩ H:\", len(test_rel  & H_files))\n",
    "\n",
    "\n",
    "MAX_LEN        = int(MAX_SECONDS * TARGET_SR)\n",
    "MAX_LEN_VERIFY = int(MAX_SECONDS_VERIFY * TARGET_SR)\n",
    "_resamp = {}\n",
    "\n",
    "def load_wav(path, target_sr=TARGET_SR, max_len=MAX_LEN, crop_train=True):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.shape[0] > 1: wav = wav.mean(0, keepdim=True)\n",
    "    if sr != target_sr:\n",
    "        if sr not in _resamp: _resamp[sr] = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        wav = _resamp[sr](wav)\n",
    "    if wav.shape[-1] > max_len:\n",
    "        if crop_train:\n",
    "            start = torch.randint(0, wav.shape[-1]-max_len+1, (1,)).item()\n",
    "            wav = wav[:, start:start+max_len]\n",
    "        else:\n",
    "            st = (wav.shape[-1]-max_len)//2\n",
    "            wav = wav[:, st:st+max_len]\n",
    "    return wav\n",
    "\n",
    "def aug_wav(wav):\n",
    "    if torch.rand(1).item() < 0.6:\n",
    "        g = 10 ** (float(torch.empty(1).uniform_(-8, 6)) / 20)\n",
    "        wav = (wav * g).clamp(-1, 1)\n",
    "    if torch.rand(1).item() < 0.6:\n",
    "        snr_db = float(torch.empty(1).uniform_(5, 20))\n",
    "        sig_pwr = wav.pow(2).mean().clamp_min(1e-9)\n",
    "        noise_pwr = sig_pwr / (10 ** (snr_db / 10))\n",
    "        noise = torch.randn_like(wav)\n",
    "        noise = noise * (noise_pwr.sqrt() / (noise.pow(2).mean().clamp_min(1e-9).sqrt()))\n",
    "        wav = (wav + noise).clamp(-1, 1)\n",
    "    if torch.rand(1).item() < 0.4:\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            cutoff = float(torch.empty(1).uniform_(2500, 4500))\n",
    "            wav = torchaudio.functional.lowpass_biquad(wav, TARGET_SR, cutoff)\n",
    "        else:\n",
    "            cutoff = float(torch.empty(1).uniform_(100, 250))\n",
    "            wav = torchaudio.functional.highpass_biquad(wav, TARGET_SR, cutoff)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        max_shift = int(0.08 * TARGET_SR)\n",
    "        shift = int(torch.randint(-max_shift, max_shift + 1, (1,)).item())\n",
    "        if shift > 0:\n",
    "            wav = torch.cat([torch.zeros(1, shift, device=wav.device), wav[..., :-shift]], dim=-1)\n",
    "        elif shift < 0:\n",
    "            sh = -shift\n",
    "            wav = torch.cat([wav[..., sh:], torch.zeros(1, sh, device=wav.device)], dim=-1)\n",
    "    if wav.shape[-1] > MAX_LEN: wav = wav[..., :MAX_LEN]\n",
    "    return wav\n",
    "\n",
    "def maybe_mixup(wav, y, pool_pairs, alpha=(0.2,0.8)):\n",
    "    if torch.rand(1).item() >= 0.3: return wav, y\n",
    "    p2, y2 = random.choice(pool_pairs)\n",
    "    w2 = load_wav(p2, crop_train=True)\n",
    "    if w2.shape[-1] < wav.shape[-1]:\n",
    "        pad = torch.zeros(1, wav.shape[-1]-w2.shape[-1])\n",
    "        w2 = torch.cat([w2, pad], dim=-1)\n",
    "    elif w2.shape[-1] > wav.shape[-1]:\n",
    "        w2 = w2[..., :wav.shape[-1]]\n",
    "    a = float(torch.empty(1).uniform_(*alpha))\n",
    "    wav = (a*wav + (1-a)*w2).clamp(-1, 1)\n",
    "    return wav, y\n",
    "\n",
    "FREQ_MASK_PARAM = 18\n",
    "TIME_MASK_PARAM = 32\n",
    "FREQ_MASKS = 2\n",
    "TIME_MASKS = 2\n",
    "\n",
    "def spec_augment_(feat):\n",
    "\n",
    "    x = feat.transpose(1,2) \n",
    "    for _ in range(FREQ_MASKS):\n",
    "        x = torchaudio.transforms.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM)(x)\n",
    "    for _ in range(TIME_MASKS):\n",
    "        x = torchaudio.transforms.TimeMasking(time_mask_param=TIME_MASK_PARAM)(x)\n",
    "    return x.transpose(1,2)\n",
    "\n",
    "\n",
    "class VNCelebID(Dataset):\n",
    "    def __init__(self, pairs, train=True, pool_pairs=None):\n",
    "        self.pairs = pairs; self.train = train\n",
    "        self.pool_pairs = pool_pairs if pool_pairs is not None else pairs\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        path, y = self.pairs[i]\n",
    "        wav = load_wav(path, crop_train=self.train)\n",
    "        if self.train:\n",
    "            wav = aug_wav(wav)\n",
    "            wav, y = maybe_mixup(wav, y, self.pool_pairs)\n",
    "        return wav, y\n",
    "\n",
    "def pad_batch(batch):\n",
    "    wavs, ys = zip(*batch)\n",
    "    maxlen = max(w.shape[-1] for w in wavs)\n",
    "    out = []\n",
    "    for w in wavs:\n",
    "        if w.shape[-1] < maxlen:\n",
    "            pad = torch.zeros(1, maxlen - w.shape[-1])\n",
    "            w = torch.cat([w, pad], dim=-1)\n",
    "        out.append(w)\n",
    "    wavs = torch.stack(out, dim=0) \n",
    "    ys = torch.tensor(ys, dtype=torch.long)\n",
    "    return wavs, ys\n",
    "\n",
    "SAFE_BATCH = BATCH_SIZE\n",
    "dl_tr = DataLoader(\n",
    "    VNCelebID(train_pairs, True, pool_pairs=train_pairs),\n",
    "    batch_size=SAFE_BATCH, shuffle=True,\n",
    "    num_workers=min(NUM_WORKERS, 2),\n",
    "    collate_fn=pad_batch,\n",
    "    pin_memory=False, persistent_workers=False,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "dl_va = DataLoader(VNCelebID(val_pairs, False),  batch_size=SAFE_BATCH, shuffle=False, num_workers=0, collate_fn=pad_batch)\n",
    "dl_te = DataLoader(VNCelebID(test_pairs, False), batch_size=SAFE_BATCH, shuffle=False, num_workers=0, collate_fn=pad_batch)\n",
    "print(\"Dataloaders ready. BATCH_SIZE:\", SAFE_BATCH)\n",
    "\n",
    "def iterate_steps(dloader, steps=None):\n",
    "    if steps is None:\n",
    "        for b in dloader: yield b\n",
    "    else:\n",
    "        cyc = itertools.cycle(dloader)\n",
    "        for _ in range(steps): yield next(cyc)\n",
    "\n",
    "\n",
    "class AAMSoftmax(nn.Module):\n",
    "    def __init__(self, emb_dim, num_classes, s=30.0, m=0.3):  \n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(emb_dim, num_classes))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        self.s = s; self.m = m\n",
    "    def forward(self, emb, y):\n",
    "        W = F.normalize(self.W, dim=0)\n",
    "        logits = emb @ W\n",
    "        y_onehot = F.one_hot(y, num_classes=W.size(1)).float()\n",
    "        theta = torch.acos(torch.clamp(logits, -1+1e-7, 1-1e-7))\n",
    "        target_logits = torch.cos(theta + self.m)\n",
    "        logits = logits * (1 - y_onehot) + target_logits * y_onehot\n",
    "        logits = logits * self.s\n",
    "        return logits\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_ff), nn.SiLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "        self.scale = 0.5\n",
    "    def forward(self, x):\n",
    "        return x + self.scale * self.net(x)\n",
    "\n",
    "class MHSA(nn.Module):\n",
    "    def __init__(self, d_model, nhead, p=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=p, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        q = k = v = self.ln(x)\n",
    "        out, _ = self.attn(q, k, v, key_padding_mask=key_padding_mask, need_weights=False)\n",
    "        return x + self.dropout(out)\n",
    "\n",
    "class ConvModule(nn.Module):\n",
    "    def __init__(self, d_model, k=31, p=0.1):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.pw1 = nn.Conv1d(d_model, 2*d_model, kernel_size=1)\n",
    "        self.dw  = nn.Conv1d(d_model, d_model, kernel_size=k, padding=k//2, groups=d_model)\n",
    "        self.bn  = nn.BatchNorm1d(d_model)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.pw2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "    def forward(self, x, mask=None):\n",
    "        y = self.ln(x).transpose(1,2)\n",
    "        y = F.glu(self.pw1(y), dim=1)\n",
    "        y = self.dw(y); y = self.bn(y); y = self.swish(y); y = self.pw2(y)\n",
    "        y = y.transpose(1,2)\n",
    "        y = self.dropout(y)\n",
    "        if mask is not None:\n",
    "            y = y.masked_fill(mask.unsqueeze(-1), 0.0)\n",
    "        return x + y\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, k=31, p=0.1):\n",
    "        super().__init__()\n",
    "        self.ff1 = FFN(d_model, d_ff, p)\n",
    "        self.mha = MHSA(d_model, nhead, p)\n",
    "        self.conv= ConvModule(d_model, k, p)\n",
    "        self.ff2 = FFN(d_model, d_ff, p)\n",
    "        self.ln  = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        x = self.ff1(x)\n",
    "        x = self.mha(x, key_padding_mask)\n",
    "        x = self.conv(x, key_padding_mask)\n",
    "        x = self.ff2(x)\n",
    "        return self.ln(x)\n",
    "\n",
    "def lengths_to_mask(lengths, max_len=None):\n",
    "    B = lengths.size(0)\n",
    "    T = int(max_len) if max_len is not None else int(lengths.max().item())\n",
    "    idx = torch.arange(T, device=lengths.device).unsqueeze(0).expand(B, T)\n",
    "    return idx >= lengths.unsqueeze(1)\n",
    "\n",
    "class MFAConformerEmbedder(nn.Module):\n",
    "\n",
    "    def __init__(self, bundle, d_model, nhead, d_ff, n_layers, emb_dim, p=0.1, k=31):\n",
    "        super().__init__()\n",
    "        self.ssl = bundle.get_model()\n",
    "        for p_ in self.ssl.parameters(): p_.requires_grad = False\n",
    "        self.ms_proj = nn.LazyLinear(d_model)\n",
    "        self.layers  = nn.ModuleList([ConformerBlock(d_model, nhead, d_ff, k=k, p=p) for _ in range(n_layers)])\n",
    "        hidden_fc = 1024 if (2 * n_layers * d_model >= 8192) else 512\n",
    "        self.emb_head = nn.Sequential(\n",
    "            nn.Linear(2 * d_model * n_layers, hidden_fc), nn.ReLU(),\n",
    "            nn.Linear(hidden_fc, emb_dim)\n",
    "        )\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model  = d_model\n",
    "\n",
    "    def forward(self, wav, apply_specaug=True):\n",
    "        if wav.dim() == 3:\n",
    "            wav = wav.squeeze(1)\n",
    "\n",
    "        with autocast('cuda', enabled=(DEVICE=='cuda')):\n",
    "            feats_list, lengths = self.ssl.extract_features(wav)  # list or tensor (B,T,Ci)\n",
    "            hs = torch.cat(feats_list, dim=-1) if isinstance(feats_list, list) else feats_list\n",
    "            hs = self.ms_proj(hs)  # (B,T,d_model)\n",
    "\n",
    "        hs = hs.float()  # AMP safety\n",
    "\n",
    "        T = hs.size(1)\n",
    "        if lengths is None:\n",
    "            lens = torch.full((hs.size(0),), T, device=hs.device, dtype=torch.long)\n",
    "        else:\n",
    "            lens = lengths.to(hs.device, dtype=torch.long).clamp_max(T)\n",
    "        pad_mask = lengths_to_mask(lens, max_len=T)\n",
    "\n",
    "        # Light SpecAugment on SSL features (train only)\n",
    "        if self.training and apply_specaug:\n",
    "            hs = spec_augment_(hs)\n",
    "\n",
    "        h = hs\n",
    "        hidden = []\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, key_padding_mask=pad_mask)\n",
    "            hidden.append(h)\n",
    "        ms = torch.cat(hidden, dim=-1)  # (B,T,L*d_model)\n",
    "\n",
    "        if pad_mask.any():\n",
    "            valid = (~pad_mask).float().unsqueeze(-1)\n",
    "            sumv  = (ms * valid).sum(dim=1)\n",
    "            cnt   = valid.sum(dim=1).clamp_min(1.0)\n",
    "            mean  = sumv / cnt\n",
    "            var   = ((ms - mean.unsqueeze(1))**2 * valid).sum(dim=1) / cnt\n",
    "            std   = var.clamp_min(1e-5).sqrt()\n",
    "        else:\n",
    "            mean = ms.mean(dim=1)\n",
    "            std  = ms.std(dim=1).clamp_min(1e-5)\n",
    "        stats = torch.cat([mean, std], dim=-1)\n",
    "\n",
    "        emb = self.emb_head(stats)\n",
    "        return F.normalize(emb, p=2, dim=-1)\n",
    "\n",
    "\n",
    "bundle = TAP.WAVLM_LARGE if USE_WAVLM_LARGE else TAP.WAVLM_BASE\n",
    "mfa = MFAConformerEmbedder(\n",
    "    bundle=bundle,\n",
    "    d_model=P[\"d_model\"], nhead=P[\"nhead\"], d_ff=P[\"d_ff\"],\n",
    "    n_layers=P[\"n_layers\"], emb_dim=P[\"emb_dim\"], k=P[\"k\"]\n",
    ").to(DEVICE)\n",
    "head_ce  = nn.Linear(P[\"emb_dim\"], num_classes).to(DEVICE)\n",
    "head_aam = AAMSoftmax(emb_dim=P[\"emb_dim\"], num_classes=num_classes, s=30.0, m=0.3).to(DEVICE)\n",
    "\n",
    "@torch.no_grad()\n",
    "def logits_aam_infer(emb):\n",
    "    Wn = F.normalize(head_aam.W, dim=0)\n",
    "    return emb @ Wn * head_aam.s\n",
    "\n",
    "def forward_logits_ce(batch_wav):\n",
    "    x = batch_wav.to(DEVICE, dtype=torch.float32)\n",
    "    e = mfa(x); return head_ce(e)\n",
    "\n",
    "def forward_logits_aam(batch_wav, y):\n",
    "    x = batch_wav.to(DEVICE, dtype=torch.float32)\n",
    "    e = mfa(x); return head_aam(e, y), e\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_batch(batch_wav):\n",
    "    x = batch_wav.to(DEVICE, dtype=torch.float32)\n",
    "    e = mfa(x)\n",
    "    return e.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def resolve_rel(rel):\n",
    "    rel = rel.strip().replace(\"\\\\\",\"/\").lstrip(\"./\")\n",
    "    if rel.startswith(\"data/\"): rel = rel.split(\"data/\", 1)[1]\n",
    "    return os.path.join(AUDIO_DIR, rel)\n",
    "\n",
    "EMBED_CACHE = {}\n",
    "MODEL_REV = 0\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_path(path, n_crops=2):\n",
    "    key = (MODEL_REV, EER_CROP_MODE, path)\n",
    "    if key in EMBED_CACHE: return EMBED_CACHE[key]\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.shape[0] > 1: wav = wav.mean(0, keepdim=True)\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.transforms.Resample(sr, TARGET_SR)(wav)\n",
    "    L = wav.shape[-1]; seg = MAX_LEN_VERIFY\n",
    "    embs = []\n",
    "    if EER_CROP_MODE == \"center\":\n",
    "        if L < seg:\n",
    "            pad = torch.zeros(1, seg - L); crop = torch.cat([wav, pad], dim=-1)\n",
    "        else:\n",
    "            st = (L - seg) // 2; crop = wav[:, st:st+seg]\n",
    "        x = crop.unsqueeze(0)\n",
    "        e = embed_batch(x)[0]; embs.append(e)\n",
    "    else:\n",
    "        if L <= seg:\n",
    "            pad = torch.zeros(1, seg - L); base = torch.cat([wav, pad], dim=-1)\n",
    "            crops = [base, base]\n",
    "        else:\n",
    "            st = (L - seg) // 2\n",
    "            crops = [wav[:, st:st+seg], wav[:, int(torch.randint(0, L - seg + 1, (1,)).item()):][:, :seg]]\n",
    "        for c in crops:\n",
    "            x = c.unsqueeze(0); e = embed_batch(x)[0]; embs.append(e)\n",
    "    e = np.mean(np.stack(embs, 0), axis=0); e = e / (np.linalg.norm(e) + 1e-9)\n",
    "    EMBED_CACHE[key] = e\n",
    "    return e\n",
    "\n",
    "def read_pairs(txt_path):\n",
    "    pairs = []\n",
    "    if (txt_path is None) or (not os.path.exists(txt_path)): return pairs\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln or ln.startswith(\"#\"): continue\n",
    "            parts = ln.split()\n",
    "            if len(parts) < 3: continue\n",
    "            lab = int(parts[0]); a = parts[1]; b = parts[2]\n",
    "            pairs.append((lab, a, b))\n",
    "    return pairs\n",
    "\n",
    "print(\"Building cohort (initial) ...\")\n",
    "def build_cohort(paths, K=2000, seed=7):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if not paths: return np.zeros((0, P[\"emb_dim\"]), dtype=np.float32)\n",
    "    idxs = rng.choice(len(paths), size=min(K, len(paths)), replace=False)\n",
    "    E = []\n",
    "    for i in idxs:\n",
    "        p = paths[i]\n",
    "        if os.path.exists(p):\n",
    "            e = embed_path(p); E.append(e)\n",
    "    if len(E) == 0: return np.zeros((0, P[\"emb_dim\"]), dtype=np.float32)\n",
    "    return np.stack(E, 0)\n",
    "\n",
    "COHORT = build_cohort(kept_abs, K=COHORT_K_INIT)\n",
    "print(\"Cohort size:\", COHORT.shape)\n",
    "\n",
    "def s_norm_score(e1, e2, cohort=COHORT, topM=SNORM_TOPM):\n",
    "    if (not USE_SNORM) or (cohort is None) or getattr(cohort, \"shape\", (0,))[0] == 0:\n",
    "        return float(e1 @ e2)\n",
    "    c1 = cohort @ e1; c2 = cohort @ e2\n",
    "    m1 = np.sort(c1)[-min(topM, c1.shape[0]):].mean()\n",
    "    m2 = np.sort(c2)[-min(topM, c2.shape[0]):].mean()\n",
    "    return float(e1 @ e2) - 0.5 * (m1 + m2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_eer_for_pairs(txt_path, max_pairs=None):\n",
    "    pairs = read_pairs(txt_path)\n",
    "    if len(pairs) == 0: return None, None, 0, 0\n",
    "    if (max_pairs is not None) and (len(pairs) > max_pairs):\n",
    "        rng = np.random.default_rng(123)\n",
    "        idxs = rng.choice(len(pairs), size=max_pairs, replace=False)\n",
    "        pairs = [pairs[i] for i in idxs]\n",
    "    scores, labels, skipped = [], [], 0\n",
    "    for lab, a_rel, b_rel in pairs:\n",
    "        pa = resolve_rel(a_rel); pb = resolve_rel(b_rel)\n",
    "        if not (os.path.exists(pa) and os.path.exists(pb)):\n",
    "            skipped += 1; continue\n",
    "        ea = embed_path(pa); eb = embed_path(pb)\n",
    "        scores.append(s_norm_score(ea, eb)); labels.append(lab)\n",
    "    if len(scores) == 0: return None, None, 0, skipped\n",
    "    labels = np.array(labels); scores = np.array(scores)\n",
    "    fpr, tpr, th = roc_curve(labels, scores, pos_label=1)\n",
    "    fnr = 1 - tpr; i = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer = float((fnr[i] + fpr[i]) / 2.0); thr = float(th[i])\n",
    "    return eer, thr, len(scores), skipped\n",
    "\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "scaler = GradScaler('cuda') if DEVICE == \"cuda\" else None\n",
    "\n",
    "def run_epoch_lp(dloader, opt, steps=None):\n",
    "    mfa.train(); head_ce.train(); head_aam.eval()\n",
    "    tot, correct, loss_sum = 0, 0, 0.0\n",
    "    for it, (wavs, ys) in enumerate(iterate_steps(dloader, steps)):\n",
    "        ys = ys.to(DEVICE, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with autocast('cuda', enabled=(DEVICE=='cuda')):\n",
    "            logits = forward_logits_ce(wavs)\n",
    "            loss = ce(logits, ys)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(mfa.parameters())+list(head_ce.parameters()), 5.0)\n",
    "        scaler.step(opt); scaler.update()\n",
    "        preds = logits.argmax(1); bs = ys.size(0)\n",
    "        correct += (preds == ys).sum().item(); tot += bs\n",
    "        loss_sum += float(loss.item()) * bs\n",
    "        if DEVICE == \"cuda\" and (it % 50 == 0): torch.cuda.empty_cache()\n",
    "    return loss_sum / max(1, tot), correct / max(1, tot)\n",
    "\n",
    "def run_epoch_hybrid(dloader, opt, lambda_aam=0.7, steps=None):\n",
    "    mfa.train(); head_ce.train(); head_aam.train()\n",
    "    tot, correct, loss_sum = 0, 0, 0.0\n",
    "    for it, (wavs, ys) in enumerate(iterate_steps(dloader, steps)):\n",
    "        ys = ys.to(DEVICE, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        x = wavs.to(DEVICE, dtype=torch.float32)\n",
    "        with autocast('cuda', enabled=(DEVICE=='cuda')):\n",
    "            e = mfa(x, apply_specaug=True)\n",
    "            logits_ce  = head_ce(e)\n",
    "            logits_aam = head_aam(e, ys)\n",
    "            loss = (1 - lambda_aam) * ce(logits_ce, ys) + lambda_aam * ce(logits_aam, ys)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(mfa.parameters())+list(head_ce.parameters())+list(head_aam.parameters()), 5.0)\n",
    "        scaler.step(opt); scaler.update()\n",
    "        preds = logits_ce.argmax(1)\n",
    "        bs = ys.size(0)\n",
    "        correct += (preds == ys).sum().item(); tot += bs\n",
    "        loss_sum += float(loss.item()) * bs\n",
    "        if DEVICE == \"cuda\" and (it % 50 == 0): torch.cuda.empty_cache()\n",
    "    return loss_sum / max(1, tot), correct / max(1, tot)\n",
    "\n",
    "def run_epoch_aam(dloader, opt=None, train=True, steps=None):\n",
    "    if train: mfa.train(); head_aam.train(); head_ce.eval()\n",
    "    else:     mfa.eval();  head_aam.eval();  head_ce.eval()\n",
    "    tot, correct, loss_sum = 0, 0, 0.0\n",
    "    for it, (wavs, ys) in enumerate(iterate_steps(dloader, steps if train else None)):\n",
    "        ys = ys.to(DEVICE, non_blocking=True)\n",
    "        if train: opt.zero_grad(set_to_none=True)\n",
    "        x = wavs.to(DEVICE, dtype=torch.float32)\n",
    "        with autocast('cuda', enabled=(DEVICE=='cuda')):\n",
    "            e = mfa(x, apply_specaug=train)\n",
    "            logits_train = head_aam(e, ys)\n",
    "            loss = ce(logits_train, ys) if train else torch.tensor(0., device=x.device)\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(list(mfa.parameters())+list(head_aam.parameters()), 5.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "        with torch.no_grad():\n",
    "            preds = logits_aam_infer(e).argmax(1)\n",
    "        bs = ys.size(0)\n",
    "        correct += (preds == ys).sum().item(); tot += bs\n",
    "        loss_sum += float(loss.item()) * bs\n",
    "        if DEVICE == \"cuda\" and (it % 50 == 0): torch.cuda.empty_cache()\n",
    "    return loss_sum / max(1, tot), correct / max(1, tot)\n",
    "\n",
    "def _set_requires(module, flag: bool):\n",
    "    for p in module.parameters(): p.requires_grad = flag\n",
    "\n",
    "def _unfreeze_ssl_last_k(ssl_module, k=8):  \n",
    "    try:\n",
    "        enc = ssl_module.encoder\n",
    "        layers = list(enc.transformer.layers)\n",
    "        L = len(layers)\n",
    "        for i, layer in enumerate(layers):\n",
    "            req = (i >= L - k)\n",
    "            for p in layer.parameters(): p.requires_grad = req\n",
    "        print(f\"Unfroze last-{k} WavLM layers.\")\n",
    "    except Exception:\n",
    "        _set_requires(ssl_module, True)\n",
    "        print(\"Fallback: unfroze ALL SSL layers.\")\n",
    "\n",
    "def set_trainable_lp():\n",
    "    _set_requires(mfa.ssl, False)\n",
    "    _set_requires(mfa.ms_proj, True)\n",
    "    for l in mfa.layers: _set_requires(l, True)\n",
    "    _set_requires(mfa.emb_head, True)\n",
    "    _set_requires(head_ce, True)\n",
    "    _set_requires(head_aam, False)\n",
    "\n",
    "def set_trainable_ftA():\n",
    "    _set_requires(mfa.ssl, False)\n",
    "    _set_requires(mfa.ms_proj, True)\n",
    "    for l in mfa.layers: _set_requires(l, True)\n",
    "    _set_requires(mfa.emb_head, True)\n",
    "    _set_requires(head_ce, True)\n",
    "    _set_requires(head_aam, True)\n",
    "\n",
    "def set_trainable_ftB():\n",
    "    _unfreeze_ssl_last_k(mfa.ssl, k=8)   \n",
    "    _set_requires(mfa.ms_proj, True)\n",
    "    for l in mfa.layers: _set_requires(l, True)\n",
    "    _set_requires(mfa.emb_head, True)\n",
    "    _set_requires(head_ce, False)\n",
    "    _set_requires(head_aam, True)\n",
    "\n",
    "\n",
    "best_va_acc   = -1.0\n",
    "best_path_val = \"/kaggle/working/mfa_conf_best_by_val.pt\"\n",
    "best_path_eer_e = \"/kaggle/working/mfa_conf_best_by_eer_E.pt\"\n",
    "best_path_eer_h = \"/kaggle/working/mfa_conf_best_by_eer_H.pt\"\n",
    "alias_best      = \"/kaggle/working/mfa_conf_best.pt\"\n",
    "\n",
    "best_eer_e, best_thr_e = 1.0, None\n",
    "best_eer_h, best_thr_h = 1.0, None\n",
    "GLOBAL_EER_BEST = 1.0\n",
    "\n",
    "def save_state(path, split=None, eer=None, thr=None):\n",
    "    obj = {\"mfa\": mfa.state_dict(), \"head_ce\": head_ce.state_dict(), \"head_aam\": head_aam.state_dict(), \"spk2idx\": spk2idx}\n",
    "    if split is not None: obj.update({\"best_eer\": eer, \"best_thr\": thr, \"split\": split})\n",
    "    torch.save(obj, path)\n",
    "\n",
    "def eval_eer_checkpoint(tag=\"LP\", max_pairs_E=MAX_PAIRS_LP, max_pairs_H=MAX_PAIRS_LP):\n",
    "    global EMBED_CACHE\n",
    "    EMBED_CACHE = {}\n",
    "    eer_e, thr_e, n_e, _ = compute_eer_for_pairs(TXT_E, max_pairs=max_pairs_E) if TXT_E else (None,None,0,0)\n",
    "    eer_h, thr_h, n_h, _ = compute_eer_for_pairs(TXT_H, max_pairs=max_pairs_H) if TXT_H else (None,None,0,0)\n",
    "    msg = f\"[{tag}]\"\n",
    "    if eer_e is not None: msg += f\" EER_E={eer_e*100:.2f}%({n_e})\"\n",
    "    if eer_h is not None: msg += f\" | EER_H={eer_h*100:.2f}%({n_h})\"\n",
    "    print(msg)\n",
    "    return eer_e, thr_e, eer_h, thr_h\n",
    "\n",
    "def maybe_update_best(eer_e, thr_e, eer_h, thr_h):\n",
    "    global best_eer_e, best_thr_e, best_eer_h, best_thr_h, GLOBAL_EER_BEST\n",
    "    updated = False\n",
    "    if (eer_e is not None) and (eer_e < best_eer_e):\n",
    "        best_eer_e, best_thr_e = eer_e, thr_e\n",
    "        save_state(best_path_eer_e, split=\"E\", eer=best_eer_e, thr=best_thr_e)\n",
    "        updated = True\n",
    "        if best_eer_e < GLOBAL_EER_BEST:\n",
    "            shutil.copy(best_path_eer_e, alias_best)\n",
    "            GLOBAL_EER_BEST = best_eer_e\n",
    "            print(f\" [alias] mfa_conf_best.pt -> E (EER={best_eer_e*100:.2f}%)\")\n",
    "    if (eer_h is not None) and (eer_h < best_eer_h):\n",
    "        best_eer_h, best_thr_h = eer_h, thr_h\n",
    "        save_state(best_path_eer_h, split=\"H\", eer=best_eer_h, thr=best_thr_h)\n",
    "        updated = True\n",
    "        if best_eer_h < GLOBAL_EER_BEST:\n",
    "            shutil.copy(best_path_eer_h, alias_best)\n",
    "            GLOBAL_EER_BEST = best_eer_h\n",
    "            print(f\" [alias] mfa_conf_best.pt -> H (EER={best_eer_h*100:.2f}%)\")\n",
    "    return updated\n",
    "\n",
    "\n",
    "\n",
    "set_trainable_lp()\n",
    "opt_lp = optim.AdamW(\n",
    "    [p for p in list(mfa.parameters()) + list(head_ce.parameters()) if p.requires_grad],\n",
    "    lr=2e-3, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "def build_warmup_cosine(optimizer, warmup_steps, total_steps, min_lr=1e-6, base_lr=2e-3):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return (step + 1) / max(1, warmup_steps)\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return max(min_lr / base_lr, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "total_steps_lp = E_LP * STEPS_PER_EPOCH_TRAIN\n",
    "sched_lp = build_warmup_cosine(opt_lp, warmup_steps=int(0.05*total_steps_lp), total_steps=total_steps_lp, base_lr=2e-3)\n",
    "\n",
    "MODEL_REV = 0\n",
    "step_ctr = 0\n",
    "for ep in range(1, E_LP+1):\n",
    "    tr_loss, tr_acc = run_epoch_lp(dl_tr, opt_lp, steps=STEPS_PER_EPOCH_TRAIN)\n",
    "    step_ctr += STEPS_PER_EPOCH_TRAIN\n",
    "    sched_lp.step()\n",
    "\n",
    "\n",
    "    mfa.eval(); head_ce.eval()\n",
    "    with torch.no_grad():\n",
    "        tot, correct = 0, 0\n",
    "        for wavs, ys in iterate_steps(dl_va, steps=300 if FAST_MODE else None):\n",
    "            ys = ys.to(DEVICE, non_blocking=True)\n",
    "            logits = forward_logits_ce(wavs)\n",
    "            correct += (logits.argmax(1) == ys).sum().item()\n",
    "            tot += ys.size(0)\n",
    "        va_acc = correct / max(1, tot)\n",
    "    if va_acc > best_va_acc: best_va_acc = va_acc; save_state(best_path_val)\n",
    "    if ep % EER_EVAL_EVERY_LP == 0:\n",
    "        eer_e, thr_e, eer_h, thr_h = eval_eer_checkpoint(tag=f\"LP-Ep{ep}\", max_pairs_E=MAX_PAIRS_LP, max_pairs_H=MAX_PAIRS_LP)\n",
    "        maybe_update_best(eer_e, thr_e, eer_h, thr_h)\n",
    "    MODEL_REV += 1\n",
    "    print(f\"[LP] Ep{ep:02d} | tr_acc={tr_acc:.4f} | va_acc={va_acc:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Rebuilding cohort for S-Norm after LP ...\")\n",
    "EMBED_CACHE = {}; COHORT = build_cohort(kept_abs, K=COHORT_K_FT)\n",
    "print(\"Cohort size (rebuild):\", COHORT.shape)\n",
    "\n",
    "\n",
    "set_trainable_ftA()\n",
    "paramsA = [p for p in list(mfa.parameters())+list(head_ce.parameters())+list(head_aam.parameters()) if p.requires_grad]\n",
    "optA = optim.AdamW(paramsA, lr=5e-5, weight_decay=1e-5)\n",
    "total_steps_fta = E_A * STEPS_PER_EPOCH_TRAIN\n",
    "schedA = torch.optim.lr_scheduler.CosineAnnealingLR(optA, T_max=max(10, E_A))\n",
    "\n",
    "for ep in range(1, E_A+1):\n",
    "    tr_loss, tr_acc = run_epoch_hybrid(dl_tr, optA, lambda_aam=0.7, steps=STEPS_PER_EPOCH_TRAIN)\n",
    "    _, va_acc = run_epoch_aam(dl_va, opt=None, train=False)\n",
    "    if va_acc > best_va_acc + 1e-4: best_va_acc = va_acc; save_state(best_path_val)\n",
    "    eer_e, thr_e, eer_h, thr_h = eval_eer_checkpoint(tag=f\"FT-A{ep:02d}\", max_pairs_E=MAX_PAIRS_FT, max_pairs_H=MAX_PAIRS_FT)\n",
    "    maybe_update_best(eer_e, thr_e, eer_h, thr_h)\n",
    "    schedA.step(); MODEL_REV += 1\n",
    "    print(f\"[FT-A] Ep{ep:02d} | tr_acc={tr_acc:.4f} | va_acc={va_acc:.4f}\")\n",
    "\n",
    "\n",
    "set_trainable_ftB()\n",
    "paramsB = [p for p in list(mfa.parameters())+list(head_aam.parameters()) if p.requires_grad]\n",
    "optB   = optim.AdamW(paramsB, lr=8e-6, weight_decay=1e-5)  \n",
    "schedB = optim.lr_scheduler.CosineAnnealingLR(optB, T_max=max(E_B, 20))\n",
    "\n",
    "no_improve = 0\n",
    "for ep in range(1, E_B+1):\n",
    "    tr_loss, tr_acc = run_epoch_aam(dl_tr, optB, train=True, steps=STEPS_PER_EPOCH_TRAIN)\n",
    "    _, va_acc = run_epoch_aam(dl_va, opt=None, train=False)\n",
    "    improved = False\n",
    "    if va_acc > best_va_acc + 2e-4:\n",
    "        best_va_acc = va_acc\n",
    "        save_state(best_path_val)\n",
    "        improved = True\n",
    "\n",
    "    eer_e, thr_e, eer_h, thr_h = eval_eer_checkpoint(\n",
    "        tag=f\"FT-B{ep:02d}\",\n",
    "        max_pairs_E=MAX_PAIRS_FT,\n",
    "        max_pairs_H=MAX_PAIRS_FT\n",
    "    )\n",
    "    if maybe_update_best(eer_e, thr_e, eer_h, thr_h):\n",
    "        improved = True\n",
    "\n",
    "    schedB.step(); MODEL_REV += 1\n",
    "    no_improve = 0 if improved else (no_improve + 1)\n",
    "    print(f\"[FT-B] Ep{ep:02d} | va_acc={va_acc:.4f} | best_eer={min(best_eer_e, best_eer_h)*100:.2f}% | no_improve={no_improve}\")\n",
    "    if no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(\"Early stop FT-B due to no improvement.\")\n",
    "        break\n",
    "\n",
    "gc.collect()\n",
    "if DEVICE == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n== Checkpoint summary (MFA-Conformer) ==\")\n",
    "print(\"Best by VAL :\", best_path_val, \"| best_va_acc =\", f\"{best_va_acc:.4f}\")\n",
    "print(\"Best by EER-E:\", best_path_eer_e, \"| best_eer_e =\", f\"{best_eer_e*100:.2f}%\", \"| thr =\", best_thr_e)\n",
    "print(\"Best by EER-H:\", best_path_eer_h, \"| best_eer_h =\", f\"{best_eer_h*100:.2f}%\", \"| thr =\", best_thr_h)\n",
    "print(\"Alias (ưu tiên EER):\", alias_best, \"| current_best_eer =\", f\"{min(best_eer_e, best_eer_h)*100:.2f}%\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_logits_tta_aam_infer(batch_wav, n_crops=2):\n",
    "    B, _, T = batch_wav.shape\n",
    "    seg = min(MAX_LEN_VERIFY, T)\n",
    "    def crop_center(x):\n",
    "        if T <= seg: return x\n",
    "        st = (T - seg)//2\n",
    "        return x[..., st:st+seg]\n",
    "    def crop_random(x):\n",
    "        if T <= seg: return x\n",
    "        st = int(torch.randint(0, T - seg + 1, (1,)).item())\n",
    "        return x[..., st:st+seg]\n",
    "    crops = [crop_center(batch_wav)] + [crop_random(batch_wav)]\n",
    "    logits_sum = 0\n",
    "    for c in crops:\n",
    "        if c.shape[-1] < MAX_LEN_VERIFY:\n",
    "            pad = torch.zeros(B,1,MAX_LEN_VERIFY - c.shape[-1], device=c.device, dtype=c.dtype)\n",
    "            c = torch.cat([c,pad], dim=-1)\n",
    "        x = c.to(DEVICE, dtype=torch.float32)\n",
    "        e = mfa(x, apply_specaug=False)\n",
    "        logits_sum = logits_sum + logits_aam_infer(e)\n",
    "    return logits_sum / float(len(crops))\n",
    "\n",
    "def eval_loader_aam_tta(dloader, n_crops=2):\n",
    "    mfa.eval(); head_aam.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for wavs, ys in dloader:\n",
    "            logits = forward_logits_tta_aam_infer(wavs, n_crops=n_crops)\n",
    "            preds = logits.argmax(1).cpu().numpy().tolist()\n",
    "            y_pred += preds; y_true += ys.numpy().tolist()\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def safe_load_meta(path):\n",
    "    if not os.path.exists(path): return None, math.inf, None\n",
    "    obj = torch.load(path, map_location=DEVICE)\n",
    "    return obj, obj.get(\"best_eer\", math.inf), obj.get(\"best_thr\", None)\n",
    "\n",
    "obj_e, eer_e, thr_e = safe_load_meta(best_path_eer_e)\n",
    "obj_h, eer_h, thr_h = safe_load_meta(best_path_eer_h)\n",
    "if (obj_e is not None and eer_e <= eer_h):\n",
    "    ckpt = obj_e; chosen_path = best_path_eer_e; chosen_split=\"E\"; chosen_eer=eer_e; chosen_thr=thr_e\n",
    "elif obj_h is not None:\n",
    "    ckpt = obj_h; chosen_path = best_path_eer_h; chosen_split=\"H\"; chosen_eer=eer_h; chosen_thr=thr_h\n",
    "else:\n",
    "    ckpt = torch.load(best_path_val, map_location=DEVICE)\n",
    "    chosen_path = best_path_val; chosen_split=\"VAL\"; chosen_eer=None; chosen_thr=None\n",
    "\n",
    "mfa.load_state_dict(ckpt[\"mfa\"]); head_ce.load_state_dict(ckpt[\"head_ce\"]); head_aam.load_state_dict(ckpt[\"head_aam\"])\n",
    "spk2idx = ckpt[\"spk2idx\"]; idx2spk = {i: s for s, i in spk2idx.items()}\n",
    "\n",
    "shutil.copy(chosen_path, alias_best)\n",
    "msg = f\"Loaded: {chosen_path} -> aliased to {alias_best}\"\n",
    "if chosen_eer is not None:\n",
    "    msg += f\" | chosen_split={chosen_split} | EER={chosen_eer*100:.2f}% | thr={chosen_thr}\"\n",
    "print(msg)\n",
    "\n",
    "val_acc  = eval_loader_aam_tta(dl_va,  n_crops=2)\n",
    "test_acc = eval_loader_aam_tta(dl_te,  n_crops=2)\n",
    "print(f\"VAL acc (AAM+TTA) = {val_acc:.4f} | TEST acc (AAM+TTA) = {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "with open(\"/kaggle/working/spk_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(idx2spk, f, ensure_ascii=False, indent=2)\n",
    "meta = {\"chosen_path\": chosen_path, \"split\": chosen_split}\n",
    "if chosen_eer is not None: meta.update({\"best_eer\": float(chosen_eer), \"best_thr\": float(chosen_thr)})\n",
    "with open(\"/kaggle/working/train_meta_mfa_conf.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "export_dir = \"/kaggle/working/export_id_model_mfa_conf\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "for p in [alias_best, best_path_val, best_path_eer_e, best_path_eer_h,\n",
    "          \"/kaggle/working/spk_map.json\", \"/kaggle/working/train_meta_mfa_conf.json\"]:\n",
    "    if os.path.exists(p): shutil.copy(p, export_dir)\n",
    "\n",
    "zip_path = \"/kaggle/working/mfa_conformer_model_improved\"\n",
    "shutil.make_archive(zip_path, 'zip', export_dir)\n",
    "print(\"Saved zip:\", zip_path + \".zip\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6096635,
     "sourceId": 9920101,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-12T03:12:11.034268",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
