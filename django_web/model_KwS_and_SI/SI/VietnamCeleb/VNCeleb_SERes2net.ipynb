{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929a105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:31:04.514726Z",
     "iopub.status.busy": "2025-10-17T10:31:04.514501Z",
     "iopub.status.idle": "2025-10-17T10:31:05.269382Z",
     "shell.execute_reply": "2025-10-17T10:31:05.268610Z"
    },
    "papermill": {
     "duration": 0.76172,
     "end_time": "2025-10-17T10:31:05.270928",
     "exception": false,
     "start_time": "2025-10-17T10:31:04.509208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Res2Net/Res2Net-PretrainedModels.git\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/Res2Net-PretrainedModels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c8b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:31:05.281533Z",
     "iopub.status.busy": "2025-10-17T10:31:05.281276Z",
     "iopub.status.idle": "2025-10-17T10:31:05.408566Z",
     "shell.execute_reply": "2025-10-17T10:31:05.407540Z"
    },
    "papermill": {
     "duration": 0.133708,
     "end_time": "2025-10-17T10:31:05.410185",
     "exception": false,
     "start_time": "2025-10-17T10:31:05.276477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!grep -E \"def |class \" /kaggle/working/Res2Net-PretrainedModels/res2net_v1b.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5494cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from res2net_v1b import res2net50_v1b_26w_4s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdc7f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:31:11.640412Z",
     "iopub.status.busy": "2025-10-17T10:31:11.640131Z",
     "iopub.status.idle": "2025-10-17T10:31:11.644662Z",
     "shell.execute_reply": "2025-10-17T10:31:11.643942Z"
    },
    "papermill": {
     "duration": 0.009914,
     "end_time": "2025-10-17T10:31:11.645741",
     "exception": false,
     "start_time": "2025-10-17T10:31:11.635827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------------------- Config / Hyperparams -------------------------\n",
    "DEFAULT_SR = 16000\n",
    "AUDIO_DURATION = 2.5\n",
    "N_MELS = 64\n",
    "N_FFT = 1024\n",
    "\n",
    "HOP_LENGTH = 256\n",
    "EMBEDDING_SIZE = 256\n",
    "\n",
    "MIN_UTTS_PER_SPK  = 5      \n",
    "SPLIT_RATIO = (0.8, 0.2)\n",
    "MAX_PAIRS_FT = 5000\n",
    "\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 64\n",
    "LR = 5e-6\n",
    "\n",
    "TXT_E = \"/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-e.txt\"\n",
    "TXT_H = \"/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-h.txt\"\n",
    "AUDIO_DIR = \"/kaggle/input/vietnam-celeb-dataset/full-dataset/data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2f75b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:31:11.653720Z",
     "iopub.status.busy": "2025-10-17T10:31:11.653476Z",
     "iopub.status.idle": "2025-10-17T10:31:11.658724Z",
     "shell.execute_reply": "2025-10-17T10:31:11.657991Z"
    },
    "papermill": {
     "duration": 0.010366,
     "end_time": "2025-10-17T10:31:11.659808",
     "exception": false,
     "start_time": "2025-10-17T10:31:11.649442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_audio(path: str, sr: int = DEFAULT_SR, duration: float = AUDIO_DURATION, random_crop: bool = False):\n",
    "    wav, orig_sr = sf.read(path)     # load file wav\n",
    "    if wav.ndim > 1:\n",
    "        wav = np.mean(wav, axis=1)   # stereo -> mono\n",
    "    \n",
    "    if orig_sr != sr:\n",
    "        wav = librosa.resample(wav.astype('float32'), orig_sr, sr)\n",
    "    \n",
    "    target_len = int(sr * duration)\n",
    "    \n",
    "    if len(wav) >= target_len:\n",
    "        if random_crop:\n",
    "            # chọn vị trí ngẫu nhiên trong file để crop\n",
    "            start = np.random.randint(0, len(wav) - target_len + 1)\n",
    "        else:\n",
    "            # lấy từ đầu file\n",
    "            start = 0\n",
    "        wav = wav[start:start + target_len]\n",
    "    else:\n",
    "        pad_width = target_len - len(wav)\n",
    "        wav = np.pad(wav, (0, pad_width), mode='constant')\n",
    "    \n",
    "    return wav.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc5322",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:31:11.667959Z",
     "iopub.status.busy": "2025-10-17T10:31:11.667492Z",
     "iopub.status.idle": "2025-10-17T10:31:11.672811Z",
     "shell.execute_reply": "2025-10-17T10:31:11.672019Z"
    },
    "papermill": {
     "duration": 0.010373,
     "end_time": "2025-10-17T10:31:11.673945",
     "exception": false,
     "start_time": "2025-10-17T10:31:11.663572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def waveform_to_features(\n",
    "    wav: torch.Tensor, \n",
    "    sr=DEFAULT_SR,\n",
    "    n_fft=N_FFT, \n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mels=N_MELS, \n",
    "    normalize=True\n",
    "):\n",
    "\n",
    "    if isinstance(wav, np.ndarray):\n",
    "        wav = torch.tensor(wav, dtype=torch.float32)\n",
    "\n",
    "    mel_spec = T.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels,\n",
    "        power=2.0\n",
    "    )(wav.unsqueeze(0))  # [1, n_mels, T]\n",
    "\n",
    "    log_mel = T.AmplitudeToDB()(mel_spec).squeeze(0)  # [n_mels, T]\n",
    "\n",
    "    if normalize:\n",
    "        log_mel = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n",
    "\n",
    "    features = log_mel.unsqueeze(0)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71e5eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:31:11.681839Z",
     "iopub.status.busy": "2025-10-17T10:31:11.681612Z",
     "iopub.status.idle": "2025-10-17T10:36:52.151215Z",
     "shell.execute_reply": "2025-10-17T10:36:52.150239Z"
    },
    "papermill": {
     "duration": 340.474902,
     "end_time": "2025-10-17T10:36:52.152404",
     "exception": false,
     "start_time": "2025-10-17T10:31:11.677502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _norm_rel(p: str) -> str:\n",
    "    p = p.strip().replace(\"\\\\\", \"/\").lstrip(\"./\")\n",
    "    if p.startswith(\"data/\"):\n",
    "        p = p.split(\"data/\", 1)[1]\n",
    "    return p\n",
    "\n",
    "def read_pairs_to_set(txt_path: str) -> set:\n",
    "    s = set()\n",
    "    if not os.path.exists(txt_path):\n",
    "        return s\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln or ln.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = ln.split()\n",
    "            if len(parts) >= 3:\n",
    "                s.add(_norm_rel(parts[1]))\n",
    "                s.add(_norm_rel(parts[2]))\n",
    "    return s\n",
    "\n",
    "E_files = read_pairs_to_set(TXT_E)\n",
    "H_files = read_pairs_to_set(TXT_H)\n",
    "ban = E_files | H_files\n",
    "print(\"Files in E:\", len(E_files), \"| Files in H:\", len(H_files), \"| BAN total:\", len(ban))\n",
    "\n",
    "EXTS = (\".wav\", \".flac\", \".mp3\", \".m4a\", \".ogg\")\n",
    "all_abs = [p for p in glob.glob(os.path.join(AUDIO_DIR, \"**\", \"*\"), recursive=True)\n",
    "           if os.path.splitext(p)[1].lower() in EXTS]\n",
    "print(\"Found audio in data/:\", len(all_abs))\n",
    "\n",
    "kept_abs = []\n",
    "for p in all_abs:\n",
    "    rel = os.path.relpath(p, AUDIO_DIR).replace(\"\\\\\", \"/\")\n",
    "    if rel in ban:\n",
    "        continue\n",
    "    kept_abs.append(p)\n",
    "print(\"Kept files after ban filter:\", len(kept_abs))\n",
    "\n",
    "spk2files_all = defaultdict(list)\n",
    "for p in kept_abs:\n",
    "    sid = os.path.basename(os.path.dirname(p))\n",
    "    spk2files_all[sid].append(p)\n",
    "print(f\"Total speakers found: {len(spk2files_all)}\")\n",
    "\n",
    "dropped_speakers = {s: fs for s, fs in spk2files_all.items() if len(fs) < MIN_UTTS_PER_SPK}\n",
    "spk2files = {s: fs for s, fs in spk2files_all.items() if len(fs) >= MIN_UTTS_PER_SPK}\n",
    "print(f\"Dropped speakers (<{MIN_UTTS_PER_SPK} utts): {len(dropped_speakers)}\")\n",
    "print(f\"Kept speakers: {len(spk2files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b1cf04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:36:52.161035Z",
     "iopub.status.busy": "2025-10-17T10:36:52.160819Z",
     "iopub.status.idle": "2025-10-17T10:36:52.205795Z",
     "shell.execute_reply": "2025-10-17T10:36:52.205013Z"
    },
    "papermill": {
     "duration": 0.050546,
     "end_time": "2025-10-17T10:36:52.206867",
     "exception": false,
     "start_time": "2025-10-17T10:36:52.156321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPLIT_TR, SPLIT_VA = SPLIT_RATIO\n",
    "random.seed(123)\n",
    "\n",
    "spk2files_tr, spk2files_va = {}, {}\n",
    "for sid, files in spk2files.items():\n",
    "    files = files[:]\n",
    "    random.shuffle(files)\n",
    "    n = len(files)\n",
    "    n_tr = max(1, int(round(SPLIT_TR * n)))\n",
    "    tr_files = files[:n_tr]\n",
    "    va_files = files[n_tr:] if n - n_tr > 0 else files[n_tr-1:]\n",
    "    spk2files_tr[sid] = tr_files\n",
    "    spk2files_va[sid] = va_files\n",
    "\n",
    "# Map speaker id -> integer label\n",
    "spk2id = {sid: idx for idx, sid in enumerate(sorted(spk2files.keys()))}\n",
    "\n",
    "# Tạo dataset: list of (file, label)\n",
    "train_set = [(f, spk2id[sid]) for sid, fs in spk2files_tr.items() for f in fs]\n",
    "val_set   = [(f, spk2id[sid]) for sid, fs in spk2files_va.items() for f in fs]\n",
    "\n",
    "print(f\"Train samples: {len(train_set)}, Val samples: {len(val_set)}\")\n",
    "print(f\"Num speakers (classes): {len(spk2id)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37769697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:36:52.215448Z",
     "iopub.status.busy": "2025-10-17T10:36:52.214988Z",
     "iopub.status.idle": "2025-10-17T10:36:52.227455Z",
     "shell.execute_reply": "2025-10-17T10:36:52.226790Z"
    },
    "papermill": {
     "duration": 0.017818,
     "end_time": "2025-10-17T10:36:52.228538",
     "exception": false,
     "start_time": "2025-10-17T10:36:52.210720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def stats(dataset, name):\n",
    "    # dataset: list of (file, label)\n",
    "    labels = [label for _, label in dataset]\n",
    "    c_label = Counter(labels)\n",
    "\n",
    "    vals = list(c_label.values())\n",
    "    if len(vals) > 0:\n",
    "        vals_sorted = sorted(vals)\n",
    "        mid = vals_sorted[len(vals_sorted) // 2]\n",
    "        print(f\"{name} - Total samples: {len(dataset)} | Speakers (classes): {len(c_label)}\")\n",
    "        print(f\"   Utterances per speaker -> Min: {min(vals)}, Median: {mid}, Max: {max(vals)}\")\n",
    "    else:\n",
    "        print(f\"{name} - Empty dataset\")\n",
    "\n",
    "stats(train_set, \"Train\")\n",
    "stats(val_set, \"Val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021392eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:36:52.236978Z",
     "iopub.status.busy": "2025-10-17T10:36:52.236791Z",
     "iopub.status.idle": "2025-10-17T10:36:52.242568Z",
     "shell.execute_reply": "2025-10-17T10:36:52.242021Z"
    },
    "papermill": {
     "duration": 0.011114,
     "end_time": "2025-10-17T10:36:52.243591",
     "exception": false,
     "start_time": "2025-10-17T10:36:52.232477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_waveform(wav, sr):\n",
    "    if random.random() < 0.5:\n",
    "        aug_type = random.choice([\"noise\", \"gain\"])\n",
    "\n",
    "        wav_tensor = torch.tensor(wav, dtype=torch.float32)\n",
    "\n",
    "        if aug_type == \"noise\":\n",
    "            noise = torch.randn_like(wav_tensor) * random.uniform(0.001, 0.01)\n",
    "            wav_tensor = wav_tensor + noise\n",
    "        elif aug_type == \"gain\":\n",
    "            gain = random.uniform(-6, 6)\n",
    "            wav_tensor = wav_tensor * (10 ** (gain / 20))\n",
    "\n",
    "        wav = wav_tensor.numpy()\n",
    "\n",
    "    return wav\n",
    "\n",
    "def spec_augment(mel):\n",
    "    mel = mel.clone()\n",
    "    freq_mask = random.randint(0, mel.size(0)//8)\n",
    "    time_mask = random.randint(0, mel.size(1)//8)\n",
    "    f0 = random.randint(0, mel.size(0)-freq_mask)\n",
    "    t0 = random.randint(0, mel.size(1)-time_mask)\n",
    "    mel[f0:f0+freq_mask, :] = 0\n",
    "    mel[:, t0:t0+time_mask] = 0\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6d187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:36:52.251875Z",
     "iopub.status.busy": "2025-10-17T10:36:52.251630Z",
     "iopub.status.idle": "2025-10-17T10:36:52.259429Z",
     "shell.execute_reply": "2025-10-17T10:36:52.258851Z"
    },
    "papermill": {
     "duration": 0.013079,
     "end_time": "2025-10-17T10:36:52.260411",
     "exception": false,
     "start_time": "2025-10-17T10:36:52.247332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TripletSpeakerDataset(Dataset):\n",
    "    def __init__(self, spk2files, sr=DEFAULT_SR, duration=AUDIO_DURATION,\n",
    "                 n_mels=N_MELS,\n",
    "                 augment=True, transform=None, random_crop=True, debug=False):\n",
    "        self.spk2files = spk2files\n",
    "        self.speakers = list(spk2files.keys())\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.n_mels = n_mels\n",
    "        self.augment = augment\n",
    "        self.transform = transform\n",
    "        self.random_crop = random_crop\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(v) for v in self.spk2files.values())\n",
    "\n",
    "    def _load_feature(self, path):\n",
    "        wav = read_audio(path, sr=self.sr, duration=self.duration, random_crop=self.random_crop)\n",
    "\n",
    "        if self.augment:\n",
    "            wav = augment_waveform(wav, self.sr)\n",
    "\n",
    "        features = waveform_to_features(\n",
    "            wav, sr=self.sr,\n",
    "            n_mels=self.n_mels,\n",
    "            normalize=True\n",
    "        )\n",
    "\n",
    "        features = F.interpolate(\n",
    "            features.unsqueeze(0),\n",
    "            size=(224, 224),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "\n",
    "        if self.augment and random.random() < 0.3:\n",
    "            features = spec_augment(features)\n",
    "\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spk = random.choice(self.speakers)\n",
    "        pos_files = self.spk2files[spk]\n",
    "\n",
    "        if len(pos_files) < 2:\n",
    "            return self.__getitem__(random.randint(0, len(self) - 1))\n",
    "\n",
    "        anchor_path, pos_path = random.sample(pos_files, 2)\n",
    "\n",
    "        neg_spk = random.choice([s for s in self.speakers if s != spk])\n",
    "        neg_path = random.choice(self.spk2files[neg_spk])\n",
    "\n",
    "        anchor = self._load_feature(anchor_path)\n",
    "        positive = self._load_feature(pos_path)\n",
    "        negative = self._load_feature(neg_path)\n",
    "\n",
    "        return anchor, positive, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a79373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:36:52.268647Z",
     "iopub.status.busy": "2025-10-17T10:36:52.268120Z",
     "iopub.status.idle": "2025-10-17T10:36:52.288280Z",
     "shell.execute_reply": "2025-10-17T10:36:52.287576Z"
    },
    "papermill": {
     "duration": 0.025352,
     "end_time": "2025-10-17T10:36:52.289394",
     "exception": false,
     "start_time": "2025-10-17T10:36:52.264042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set = [(f, sid) for sid, files in spk2files_tr.items() for f in files]\n",
    "val_set   = [(f, sid) for sid, files in spk2files_va.items() for f in files]\n",
    "\n",
    "print(\"Train samples:\", len(train_set))\n",
    "print(\"Val samples:\", len(val_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abe4bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:36:52.297907Z",
     "iopub.status.busy": "2025-10-17T10:36:52.297493Z",
     "iopub.status.idle": "2025-10-17T10:37:03.392499Z",
     "shell.execute_reply": "2025-10-17T10:37:03.391865Z"
    },
    "papermill": {
     "duration": 11.100469,
     "end_time": "2025-10-17T10:37:03.393799",
     "exception": false,
     "start_time": "2025-10-17T10:36:52.293330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.avg_pool(x)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        scale = self.sigmoid(scale)\n",
    "        return x * scale\n",
    "\n",
    "class SERes2NetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = res2net50_v1b_26w_4s(pretrained=pretrained)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.se_block = SEModule(2048, reduction=16)\n",
    "\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(2048, embedding_size),\n",
    "            nn.BatchNorm1d(embedding_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_emb=False, normalize_emb=False):\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        feat = self.backbone.forward(x)\n",
    "        feat = feat.view(feat.size(0), 2048, 1, 1)\n",
    "        feat = self.se_block(feat)\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "\n",
    "        emb = self.embedding(feat)\n",
    "        if normalize_emb:\n",
    "            emb = F.normalize(emb, p=2, dim=1)\n",
    "\n",
    "        return emb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SERes2NetEmbedding(embedding_size=256, pretrained=True).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae629ccc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:37:03.409637Z",
     "iopub.status.busy": "2025-10-17T10:37:03.408974Z",
     "iopub.status.idle": "2025-10-17T10:37:03.419868Z",
     "shell.execute_reply": "2025-10-17T10:37:03.419135Z"
    },
    "papermill": {
     "duration": 0.019639,
     "end_time": "2025-10-17T10:37:03.421003",
     "exception": false,
     "start_time": "2025-10-17T10:37:03.401364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_eer(model, txt_path, audio_dir, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pairs = []\n",
    "    skipped_missing = 0\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            label = 1 if parts[0] == \"1\" else 0\n",
    "            f1, f2 = parts[1], parts[2]\n",
    "            pairs.append((f1, f2, label))\n",
    "\n",
    "    total_pairs = len(pairs)\n",
    "    scores, labels = [], []\n",
    "\n",
    "    for f1, f2, label in pairs:\n",
    "        path1 = os.path.join(audio_dir, f1)\n",
    "        path2 = os.path.join(audio_dir, f2)\n",
    "\n",
    "        if not (os.path.exists(path1) and os.path.exists(path2)):\n",
    "            skipped_missing += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            wav1 = read_audio(path1)\n",
    "            wav2 = read_audio(path2)\n",
    "\n",
    "            feat1 = waveform_to_features(wav1, normalize=True)  # [1, n_mels, T]\n",
    "            feat2 = waveform_to_features(wav2, normalize=True)  # [1, n_mels, T]\n",
    "\n",
    "            feat1 = F.interpolate(\n",
    "                feat1.unsqueeze(0),\n",
    "                size=(224, 224),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(0)\n",
    "            feat2 = F.interpolate(\n",
    "                feat2.unsqueeze(0),\n",
    "                size=(224, 224),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(0)\n",
    "\n",
    "            if feat1.size(0) == 1:\n",
    "                feat1 = feat1.repeat(3, 1, 1)\n",
    "                feat2 = feat2.repeat(3, 1, 1)\n",
    "\n",
    "            feat1 = feat1.unsqueeze(0).to(device)\n",
    "            feat2 = feat2.unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb1 = model(feat1, return_emb=True, normalize_emb=True)\n",
    "                emb2 = model(feat2, return_emb=True, normalize_emb=True)\n",
    "                score = torch.cosine_similarity(emb1, emb2).item()\n",
    "\n",
    "            scores.append(score)\n",
    "            labels.append(label)\n",
    "\n",
    "        except Exception as e:\n",
    "            skipped_missing += 1\n",
    "            continue\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    labels = np.array(labels)\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.abs(fnr - fpr))\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "    threshold = thresholds[eer_idx]\n",
    "\n",
    "    return eer, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c470521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:37:03.435873Z",
     "iopub.status.busy": "2025-10-17T10:37:03.435238Z",
     "iopub.status.idle": "2025-10-17T10:37:03.446424Z",
     "shell.execute_reply": "2025-10-17T10:37:03.445905Z"
    },
    "papermill": {
     "duration": 0.019819,
     "end_time": "2025-10-17T10:37:03.447531",
     "exception": false,
     "start_time": "2025-10-17T10:37:03.427712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_triplet_with_eer(model, spk2files_tr, spk2files_va,\n",
    "                           epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR,\n",
    "                           weight_decay=1e-4, margin=0.3,\n",
    "                           device=None,\n",
    "                           TXT_E=TXT_E, TXT_H=TXT_H, AUDIO_DIR=AUDIO_DIR):\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_ds = TripletSpeakerDataset(spk2files_tr, random_crop=True, augment=True)\n",
    "    val_ds   = TripletSpeakerDataset(spk2files_va, random_crop=False, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=2, pin_memory=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=epochs, eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    criterion = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for anchor, positive, negative in train_loader:\n",
    "            anchor, positive, negative = (\n",
    "                anchor.to(device, non_blocking=True),\n",
    "                positive.to(device, non_blocking=True),\n",
    "                negative.to(device, non_blocking=True)\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                emb_a = model(anchor, return_emb=True, normalize_emb=True)\n",
    "                emb_p = model(positive, return_emb=True, normalize_emb=True)\n",
    "                emb_n = model(negative, return_emb=True, normalize_emb=True)\n",
    "\n",
    "                loss = criterion(emb_a, emb_p, emb_n)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        train_loss = total_loss / max(1, num_batches)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for anchor, positive, negative in val_loader:\n",
    "                anchor, positive, negative = (\n",
    "                    anchor.to(device, non_blocking=True),\n",
    "                    positive.to(device, non_blocking=True),\n",
    "                    negative.to(device, non_blocking=True)\n",
    "                )\n",
    "                with autocast():\n",
    "                    emb_a = model(anchor, return_emb=True, normalize_emb=True)\n",
    "                    emb_p = model(positive, return_emb=True, normalize_emb=True)\n",
    "                    emb_n = model(negative, return_emb=True, normalize_emb=True)\n",
    "                    loss = criterion(emb_a, emb_p, emb_n)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        val_loss /= max(1, val_batches)\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "        model.eval()\n",
    "        try:\n",
    "            eer_e, thr_e = compute_eer(model, TXT_E, audio_dir=AUDIO_DIR, device=device)\n",
    "            eer_h, thr_h = compute_eer(model, TXT_H, audio_dir=AUDIO_DIR, device=device)\n",
    "            print(f\"Epoch {epoch} | \"\n",
    "                  f\"EER_E: {eer_e:.4f}, Thr_E: {thr_e:.4f} | \"\n",
    "                  f\"EER_H: {eer_h:.4f}, Thr_H: {thr_h:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipped EER computation due to error: {e}\")\n",
    "\n",
    "        save_path = f\"triplet_se_res2net_epoch{epoch}.pth\"\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scaler_state\": scaler.state_dict(),\n",
    "        }, save_path)\n",
    "        print(f\"Saved checkpoint: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c41d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T10:37:03.462047Z",
     "iopub.status.busy": "2025-10-17T10:37:03.461555Z",
     "iopub.status.idle": "2025-10-17T21:10:30.376556Z",
     "shell.execute_reply": "2025-10-17T21:10:30.375457Z"
    },
    "papermill": {
     "duration": 38006.9236,
     "end_time": "2025-10-17T21:10:30.377851",
     "exception": false,
     "start_time": "2025-10-17T10:37:03.454251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SERes2NetEmbedding(embedding_size=256, pretrained=False).to(device)\n",
    "\n",
    "ckpt_path = \"/kaggle/input/triplet/pytorch/default/1/triplet_se_res2net_epoch7.pth\"\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "print(f\"Loaded checkpoint from {ckpt_path}\")\n",
    "\n",
    "\n",
    "train_triplet_with_eer(\n",
    "    model,\n",
    "    spk2files_tr=spk2files_tr,\n",
    "    spk2files_va=spk2files_va,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    lr=LR,\n",
    "    weight_decay=1e-4,\n",
    "    margin=0.3,\n",
    "    device=device\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6096635,
     "sourceId": 9920101,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 466260,
     "modelInstanceId": 449887,
     "sourceId": 600446,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 475083,
     "modelInstanceId": 459203,
     "sourceId": 611342,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38372.655917,
   "end_time": "2025-10-17T21:10:33.434082",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-17T10:31:00.778165",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
