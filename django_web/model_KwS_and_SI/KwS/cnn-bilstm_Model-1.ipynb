{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.functional as AF\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "N_MEL = 40\n",
    "N_MFCC = 13\n",
    "SR = 16000\n",
    "TARGET_LENGTH = 40000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "NUM_EPOCH = 50\n",
    "PATIENCE = 7\n",
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio_paths(base_path, label_map=None, default_label=None, start_count=0, show_done=True):\n",
    "    data_list = []\n",
    "    file_count = start_count\n",
    "\n",
    "    if label_map:\n",
    "        for root_folder, label in label_map.items():\n",
    "            root_path = os.path.join(base_path, root_folder)\n",
    "            if not os.path.exists(root_path):\n",
    "                continue\n",
    "\n",
    "            for dirpath, _, filenames in os.walk(root_path):\n",
    "                for file_name in filenames:\n",
    "                    if file_name.endswith((\".wav\", \".mp3\")):\n",
    "                        file_path = os.path.join(dirpath, file_name)\n",
    "                        data_list.append((file_path, label))\n",
    "                        file_count += 1\n",
    "\n",
    "    else:\n",
    "        for dirpath, _, filenames in os.walk(base_path):\n",
    "            for file_name in filenames:\n",
    "                if file_name.endswith((\".wav\", \".mp3\")):\n",
    "                    file_path = os.path.join(dirpath, file_name)\n",
    "                    data_list.append((file_path, default_label))\n",
    "                    file_count += 1\n",
    "                \n",
    "    return data_list, file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fpt_label_map = {\"true\": 1, \"false_yes\": 0, \"false_no\": 0}\n",
    "fpt_path = \"/kaggle/input/voice-fpt-aip491/Data_voices/Data_voices/FPT.AI/\"\n",
    "\n",
    "all_data_list, current_count = load_audio_paths(\n",
    "    fpt_path,\n",
    "    label_map=fpt_label_map\n",
    ")\n",
    "\n",
    "cut_sound_path = \"/kaggle/input/voice-fpt-aip491/Data_voices/Data_voices/cut_sound\"\n",
    "cut_list, current_count = load_audio_paths(\n",
    "    cut_sound_path,\n",
    "    default_label=0,\n",
    "    start_count=current_count\n",
    ")\n",
    "all_data_list.extend(cut_list)\n",
    "\n",
    "edge_base_path = \"/kaggle/input/voice-fpt-aip491/Data_voices/Data_voices/edge_voices_16k\"\n",
    "edge_label_map = {\"true\": 1, \"false_yes\": 0, \"false_no\": 0, \"false_true\": 0}\n",
    "\n",
    "for speaker_folder in os.listdir(edge_base_path):\n",
    "    speaker_path = os.path.join(edge_base_path, speaker_folder)\n",
    "    if not os.path.isdir(speaker_path): continue\n",
    "\n",
    "    temp_list, current_count = load_audio_paths(\n",
    "        speaker_path,\n",
    "        label_map=edge_label_map,\n",
    "        start_count=current_count,\n",
    "        show_done=False\n",
    "    )\n",
    "    all_data_list.extend(temp_list)\n",
    "\n",
    "print(f\"Total files found: {len(all_data_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spectrogram_transform = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=N_FFT,        \n",
    "    hop_length=HOP_LENGTH,    \n",
    "    power=2.0\n",
    ")\n",
    "\n",
    "class AudioSpectrogramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list, transform, target_length=40000, is_train=False):\n",
    "        \n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "        self.target_length = target_length\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n",
    "        self.time_masking = torchaudio.transforms.TimeMasking(time_mask_param=40)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def _augment_waveform(self, waveform):\n",
    "        if random.random() < 0.5:\n",
    "            gain = random.uniform(0.8, 1.2)\n",
    "            waveform = waveform * gain\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            noise_intensity = random.uniform(0.001, 0.005) \n",
    "            noise = torch.randn_like(waveform) * noise_intensity\n",
    "            waveform = waveform + noise\n",
    "\n",
    "        if random.random() < 0.5:\n",
    "            max_shift = int(0.1 * waveform.shape[1])\n",
    "            \n",
    "            if max_shift > 0:\n",
    "                shift_amt = random.randint(1, max_shift) \n",
    "                \n",
    "                new_waveform = torch.zeros_like(waveform)\n",
    "                length = waveform.shape[1]\n",
    "                \n",
    "                if random.random() < 0.5: \n",
    "                    new_waveform[:, shift_amt:] = waveform[:, :length - shift_amt]\n",
    "                else: \n",
    "                    new_waveform[:, :length - shift_amt] = waveform[:, shift_amt:]\n",
    "                    \n",
    "                waveform = new_waveform\n",
    "            \n",
    "        return waveform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.data_list[idx]\n",
    "        \n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        except Exception as e:\n",
    "            waveform = torch.zeros(1, self.target_length)\n",
    "\n",
    "        length = waveform.shape[1] \n",
    "\n",
    "        if length < self.target_length:\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, self.target_length - length))\n",
    "        else:\n",
    "            if self.is_train and length > self.target_length:\n",
    "                start = random.randint(0, length - self.target_length)\n",
    "                waveform = waveform[:, start:start+self.target_length]\n",
    "            else:\n",
    "                waveform = waveform[:, :self.target_length]\n",
    "\n",
    "        if self.is_train:\n",
    "            waveform = self._augment_waveform(waveform)\n",
    "\n",
    "        spec = self.transform(waveform)\n",
    "        \n",
    "        if self.is_train:\n",
    "            spec = self.freq_masking(spec)\n",
    "            spec = self.time_masking(spec)\n",
    "            \n",
    "            if random.random() < 0.3:\n",
    "                 spec = self.time_masking(spec)\n",
    "                \n",
    "        spec = spec.squeeze(0)\n",
    "        \n",
    "        return spec, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total = len(all_data_list)\n",
    "train_size = int(total * 0.9)\n",
    "\n",
    "random.shuffle(all_data_list)\n",
    "train_list = all_data_list[:train_size]\n",
    "valid_list = all_data_list[train_size:]\n",
    "\n",
    "print(f\"Train samples: {len(train_list)}\")\n",
    "print(f\"Valid samples: {len(valid_list)}\")\n",
    "\n",
    "\n",
    "train_dataset = AudioSpectrogramDataset(\n",
    "    train_list, \n",
    "    transform=spectrogram_transform, \n",
    "    target_length=TARGET_LENGTH,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "valid_dataset = AudioSpectrogramDataset(\n",
    "    valid_list, \n",
    "    transform=spectrogram_transform, \n",
    "    target_length=TARGET_LENGTH,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=0, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNN_BiLSTM(nn.Module):\n",
    "    def __init__(self,      \n",
    "                 n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "                 sr=SR, n_mfcc=N_MFCC, n_mels=N_MEL,\n",
    "                 hidden_dim=128, num_layers=2):\n",
    "        \n",
    "        super(CNN_BiLSTM, self).__init__()\n",
    "        \n",
    "        n_stft = n_fft // 2 + 1 \n",
    "        \n",
    "        self.mel_scale = torchaudio.transforms.MelScale(\n",
    "            n_mels=n_mels, \n",
    "            sample_rate=sr, \n",
    "            n_stft=n_stft\n",
    "        )\n",
    "        \n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80.0\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"dct_mat\", AF.create_dct(n_mfcc, n_mels, norm=\"ortho\"))\n",
    "        self.register_buffer(\"std_epsilon\", torch.tensor(1e-6))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3,3), padding=1)\n",
    "        self.bn1   = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d((2,2))\n",
    "        self.drop1 = nn.Dropout2d(0.3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3,3), padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d((2,2))\n",
    "        self.drop2 = nn.Dropout2d(0.3)\n",
    "\n",
    "        cnn_out_dim = 32 * (n_mfcc // 4)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_out_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
    "            batch_first=True, bidirectional=True, dropout=0.3 if num_layers > 1 else 0\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.mel_scale(x) \n",
    "        x = self.amplitude_to_db(x)\n",
    "        x = x.transpose(-1, -2)\n",
    "        x = torch.matmul(x, self.dct_mat)\n",
    "        mfcc = x.transpose(-1, -2)\n",
    "        mean = mfcc.mean(dim=-1, keepdim=True)\n",
    "        std = mfcc.std(dim=-1, keepdim=True) + self.std_epsilon\n",
    "        mfcc = (mfcc - mean) / std\n",
    "        x_cnn = mfcc.unsqueeze(1)\n",
    "        \n",
    "        x = self.drop1(self.pool1(F.relu(self.bn1(self.conv1(x_cnn)))))\n",
    "        x = self.drop2(self.pool2(F.relu(self.bn2(self.conv2(x)))))\n",
    "        \n",
    "        B, C, freq, T_reduced = x.size()\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.reshape(B, T_reduced, C * freq)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.ln(lstm_out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)\n",
    "        \n",
    "model = CNN_BiLSTM()\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def safe_auc(y_true, y_score):\n",
    "    try:\n",
    "        return roc_auc_score(y_true, y_score) if len(set(y_true)) > 1 else 0.5\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def train_model(model_class, train_loader, valid_loader, num_epochs, patience):\n",
    "    global device\n",
    "    \n",
    "    model = model_class().to(device)\n",
    "\n",
    "    print(f\"Training started with {len(train_loader.dataset)} train samples.\")\n",
    "    print(f\"Validation loader ready with {len(valid_loader.dataset)} valid samples.\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_running_loss = 0.0 \n",
    "        all_train_preds, all_train_labels, all_train_probs = [], [], []\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch).view(-1)\n",
    "            loss = criterion(outputs, y_batch.float())\n",
    "            loss.backward()\n",
    "\n",
    "            train_running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).long()\n",
    "            \n",
    "            all_train_preds.extend(preds.detach().cpu().numpy())\n",
    "            all_train_labels.extend(y_batch.detach().cpu().numpy())\n",
    "            all_train_probs.extend(probs.detach().cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"\\r   Batch {batch_idx}/{len(train_loader)}\", end=\"\")\n",
    "\n",
    "        train_loss = train_running_loss / len(train_loader.dataset)\n",
    "        train_acc = (np.array(all_train_preds) == np.array(all_train_labels)).mean()\n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average=\"macro\")\n",
    "        train_auc = safe_auc(all_train_labels, all_train_probs)\n",
    "\n",
    "        print(\"\\n   Running validation...\")\n",
    "        model.eval()\n",
    "        all_val_preds, all_val_labels, all_val_probs = [], [], []\n",
    "        valid_running_loss = 0.0 \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in valid_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val).view(-1)\n",
    "                loss = criterion(outputs, y_val.float())\n",
    "                valid_running_loss += loss.item() * X_val.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > 0.5).long()\n",
    "\n",
    "                all_val_preds.extend(preds.cpu().numpy())\n",
    "                all_val_labels.extend(y_val.cpu().numpy())\n",
    "                all_val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        valid_loss = valid_running_loss / len(valid_loader.dataset)\n",
    "        val_acc = (np.array(all_val_preds) == np.array(all_val_labels)).mean()\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average=\"macro\")\n",
    "        val_auc = safe_auc(all_val_labels, all_val_probs)\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        print(f\" RESULT | Epoch {epoch+1} | LR: {current_lr:.6f}\")\n",
    "        print(f\"   >> TRAIN | Loss: {train_loss:.4f} | Acc: {train_acc*100:.2f}% | F1: {train_f1:.3f} | AUC: {train_auc:.3f}\")\n",
    "        print(f\"   >> VALID | Loss: {valid_loss:.4f} | Acc: {val_acc*100:.2f}% | F1: {val_f1:.3f} | AUC: {val_auc:.3f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        if valid_loss < best_val_loss:\n",
    "            best_val_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_state = {\"model\": model.state_dict()}\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered. Best Val Loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    if best_state:\n",
    "        save_path = \"model_CNN_BiLSTM.pt\"\n",
    "        torch.save(best_state, save_path)\n",
    "        print(f\"Saved best model to {save_path}\")\n",
    "        model.load_state_dict(best_state[\"model\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n Starting training...\")\n",
    "trained_model = train_model(\n",
    "    model_class=CNN_BiLSTM,\n",
    "    train_loader=train_loader,   \n",
    "    valid_loader=valid_loader,\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    patience=PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_test_data_paths(test_root_dir):\n",
    "    data_list = [] \n",
    "        \n",
    "    label_map = {\"false_no\": 0, \"false_yes\": 0, \"true\": 1}\n",
    "    file_count = 0\n",
    "\n",
    "    if not os.path.exists(test_root_dir):\n",
    "        print(f\"Directory not found: {test_root_dir}\")\n",
    "        return []\n",
    "\n",
    "    for user_dir in os.listdir(test_root_dir):\n",
    "        user_path = os.path.join(test_root_dir, user_dir)\n",
    "        if not os.path.isdir(user_path):\n",
    "            continue\n",
    "\n",
    "        for sub_dir_name, label in label_map.items():\n",
    "            sub_path = os.path.join(user_path, sub_dir_name)\n",
    "            if os.path.isdir(sub_path):\n",
    "                for filename in os.listdir(sub_path):\n",
    "                    if filename.endswith((\".wav\", \".mp3\")): \n",
    "                        filepath = os.path.join(sub_path, filename)\n",
    "                        data_list.append((filepath, label))\n",
    "                        file_count += 1\n",
    "                        \n",
    "    if file_count == 0:\n",
    "        print(\"\\nNo test files found.\")\n",
    "        return []\n",
    "        \n",
    "    print(f\"\\nTotal test samples found: {file_count}\")\n",
    "    return data_list\n",
    "\n",
    "def test_model(model, test_loader, threshold=THRESHOLD, verbose=True):\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            outputs = model(X_test).view(-1)\n",
    "\n",
    "            loss = criterion(outputs, y_test.float())\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > threshold).long()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_test.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    total_samples = len(all_labels)\n",
    "    if total_samples == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    try:\n",
    "        auc_score = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.5\n",
    "    except:\n",
    "        auc_score = 0.5\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n===== TEST RESULTS =====\")\n",
    "        print(f\"Accuracy     : {accuracy:.4f}\")\n",
    "        print(f\"F1 Score     : {f1_macro:.4f}\")\n",
    "        print(f\"AUC Score    : {auc_score:.4f}\")\n",
    "    \n",
    "    return accuracy, f1_macro, auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_root_dir = \"/kaggle/input/test-aip419/Datatest/\"\n",
    "\n",
    "test_data_list = load_test_data_paths(test_root_dir)\n",
    "\n",
    "if len(test_data_list) > 0:\n",
    "    test_dataset = AudioSpectrogramDataset(\n",
    "        test_data_list, \n",
    "        transform=spectrogram_transform,\n",
    "        target_length=TARGET_LENGTH,\n",
    "        is_train=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size= 1, shuffle=False, num_workers=2) \n",
    "    \n",
    "    print(f\"Test loader is ready with {len(test_dataset)} samples.\")\n",
    "    \n",
    "    try:\n",
    "        if 'trained_model' in globals():\n",
    "            best_thresh = test_model(trained_model, test_loader)\n",
    "        else:\n",
    "            print(\"Error: 'trained_model' is not defined. Make sure you trained the model first.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR during testing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(\"No test data loaded. Skipping evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def export_to_onnx():\n",
    "    SAVED_WEIGHTS_PATH = \"model_CNN_BiLSTM.pt\"\n",
    "    ONNX_EXPORT_PATH = \"model_CNN_BiLSTM.onnx\"\n",
    "\n",
    "    print(\"Starting model export to ONNX...\")\n",
    "\n",
    "    if not os.path.exists(SAVED_WEIGHTS_PATH):\n",
    "        print(f\"Error: Weight file not found at '{SAVED_WEIGHTS_PATH}'.\")\n",
    "        print(\"Please ensure you have run the training script successfully.\")\n",
    "        return\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Initializing CNN_BiLSTM model...\")\n",
    "    model_to_export = CNN_BiLSTM(\n",
    "        sr=SR,\n",
    "        n_mfcc=N_MFCC,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MEL,\n",
    "        hidden_dim=128,\n",
    "        num_layers=2\n",
    "    ).to(device)\n",
    "\n",
    "    try:\n",
    "        print(f\"Loading weights from: {SAVED_WEIGHTS_PATH}...\")\n",
    "        checkpoint = torch.load(SAVED_WEIGHTS_PATH, map_location=device)\n",
    "\n",
    "        model_to_export.load_state_dict(checkpoint['model'])\n",
    "        print(\"Successfully loaded trained weights.\")\n",
    "\n",
    "        model_to_export.eval()\n",
    "\n",
    "        n_freq = (N_FFT // 2) + 1\n",
    "        n_time = (TARGET_LENGTH // HOP_LENGTH) + 1\n",
    "\n",
    "        dummy_input = torch.randn(1, n_freq, n_time).to(device)\n",
    "        print(f\"Creating dummy input (Spectrogram) with shape: {dummy_input.shape}\")\n",
    "\n",
    "        print(f\"Exporting model to ONNX at: {ONNX_EXPORT_PATH}...\")\n",
    "        torch.onnx.export(\n",
    "            model_to_export,\n",
    "            dummy_input,\n",
    "            ONNX_EXPORT_PATH,\n",
    "            export_params=True,\n",
    "            opset_version=17,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['spectrogram'],\n",
    "            output_names=['logits'],\n",
    "            dynamic_axes={\n",
    "                'spectrogram': {0: 'batch_size'},\n",
    "                'logits': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        print(\"-\" * 50)\n",
    "        print(f\" ONNX EXPORT SUCCESSFUL!\")\n",
    "        print(f\"Model saved at: {ONNX_EXPORT_PATH}\")\n",
    "        print(f\"Model expects input 'spectrogram' (shape [B, {n_freq}, {n_time}]) and returns 'logits'.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR DURING WEIGHT LOADING OR ONNX EXPORT:\")\n",
    "        print(f\"{type(e).__name__}: {e}\")\n",
    "        print(\"\\nPlease check if the .pt file is correct or if the model architecture matches.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_to_onnx()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8250661,
     "sourceId": 13040120,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8511916,
     "sourceId": 13411965,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8540618,
     "sourceId": 13894830,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
